{
  "common": {
    "loading": "Loading...",
    "error": "Error",
    "success": "Success",
    "save": "Save",
    "cancel": "Cancel",
    "delete": "Delete",
    "edit": "Edit",
    "create": "Create",
    "search": "Search",
    "refresh": "Refresh",
    "confirm": "Confirm",
    "back": "Back",
    "submit": "Submit",
    "reset": "Reset",
    "close": "Close",
    "open": "Open",
    "status": "Status",
    "actions": "Actions",
    "details": "Details",
    "description": "Description",
    "name": "Name",
    "name_placeholder": "Only English and numbers allowed",
    "type": "Type",
    "created_at": "Created At",
    "updated_at": "Updated At",
    "join_wechat": "Join WeChat Group",
    "join_discord": "Join Discord",
    "view_github": "View on GitHub",
    "view_docs": "View Documentation",
    "switch_theme": "Switch to {{theme}} mode",
    "light": "Light",
    "dark": "Dark",
    "toggle_sidebar": "Toggle sidebar",
    "switch_language": "Switch Language",
    "scan_qrcode": "Scan QR code to add WeChat",
    "add_wechat_note": "Note: mcp-gateway or mcpgw",
    "next": "Next",
    "filter": "Filter",
    "more": "More",
    "less": "Less",
    "all": "All",
    "none": "None",
    "yes": "Yes",
    "no": "No",
    "ok": "OK",
    "retry": "Retry",
    "upload": "Upload",
    "download": "Download",
    "preview": "Preview",
    "settings": "Settings",
    "profile": "Profile",
    "help": "Help",
    "about": "About",
    "contact": "Contact",
    "feedback": "Feedback",
    "report": "Report",
    "join_wechat_community": "Join WeChat Community",
    "join_discord_community": "Join Discord Community",
    "switch_theme_theme": "Switch to {{theme}} theme",
    "no_results": "No results found",
    "copy": "Copy",
    "copied": "Copied: {{text}}",
    "copy_failed": "Copy failed",
    "add": "Add",
    "clear": "Clear",
    "collapse": "Collapse",
    "expand": "Expand",
    "already": "Already",
    "available": "Available",
    "sse_url": "SSE URL",
    "streamable_http_url": "Streamable HTTP URL",
    "total_items": "Total {{total}} items",
    "experimental": "Experimental"
  },
  "nav": {
    "dashboard": "Dashboard",
    "settings": "Settings",
    "profile": "Profile",
    "gateway": "Gateway Config",
    "chat": "Chat Console",
    "users": "User Management",
    "tenants": "Tenant Control",
    "tools_config": "Tools Configuration",
    "sse_url": "SSE URL",
    "streamable_http_url": "Streamable HTTP URL",
    "config_versions": "Config History",
    "llm": "LLM Settings",
    "groups": {
      "chat_ai": "Chat & AI",
      "gateway_config": "Gateway & Config",
      "management": "Management"
    }
  },
  "auth": {
    "login": "Login",
    "logout": "Logout",
    "register": "Register",
    "forgot_password": "Forgot Password",
    "reset_password": "Reset Password",
    "change_password": "Change Password",
    "current_password": "Current Password",
    "new_password": "New Password",
    "confirm_password": "Confirm Password",
    "current_password_placeholder": "Enter current password",
    "new_password_placeholder": "Enter new password",
    "confirm_password_placeholder": "Confirm new password",
    "password_mismatch": "New passwords do not match",
    "password_change_success": "Password changed successfully",
    "password_change_failed": "Failed to change password, please try again",
    "confirm_change": "Confirm Change",
    "username": "Username",
    "password": "Password",
    "username_placeholder": "Enter username",
    "password_placeholder": "Enter password",
    "login_to_continue": "Please login to continue",
    "login_success": "Login successful",
    "login_failed": "Login failed, please try again",
    "logout_success": "Logout successful",
    "session_expired": "Session expired, please login again",
    "or_continue_with": "or continue with",
    "continue_with_google": "Continue with Google",
    "continue_with_github": "Continue with GitHub",
    "oauth_login_failed": "OAuth login failed"
  },
  "gateway": {
    "title": "Gateway Manager",
    "add": "Create",
    "edit": "Edit",
    "delete": "Delete",
    "export": "Export",
    "sync": "Sync",
    "import_openapi": "Import OpenAPI",
    "edit_config": "Edit MCP Server Configuration",
    "add_config": "Add New MCP Server Configuration",
    "routing_config": "Routing Configuration",
    "enabled_tools": "Enabled Tools",
    "all_tools": "All Tools",
    "mcp_config": "MCP Configuration",
    "backend_config": "MCP URL",
    "add_success": "Gateway created successfully",
    "add_failed": "Failed to create gateway",
    "edit_success": "Configuration saved successfully",
    "edit_failed": "Failed to save configuration",
    "delete_success": "Gateway deleted successfully",
    "delete_failed": "Failed to delete gateway",
    "exporting": "Exporting...",
    "export_success": "Export successful",
    "export_failed": "Export failed",
    "sync_success": "Configuration synchronized successfully",
    "sync_failed": "Failed to synchronize configuration",
    "import_success": "OpenAPI specification imported successfully",
    "import_failed": "Failed to import OpenAPI specification",
    "select_tenant": "Select Tenants",
    "search_tenant": "Search tenants...",
    "view_mode": "View Mode",
    "card_view": "Card View",
    "table_view": "Table View",
    "name": "Name",
    "name_locked": "Name cannot be changed after creation",
    "description": "Description",
    "routing": "Routing",
    "no_description": "No description available",
    "routes": "routes",
    "backends": "backends",
    "enabled": "enabled",
    "total": "total",
    "yaml_mode": "YAML Mode",
    "form_mode": "Form Mode",
    "tenant": "Tenant",
    "created_at": "Created At",
    "updated_at": "Updated At",
    "proxy_type": "Proxy Type",
    "router_config": "Router Configuration",
    "prefix": "Prefix",
    "server": "Server",
    "server_config": "Server Configuration",
    "server_name": "Server Name",
    "namespace": "Namespace",
    "allowed_tools": "Allowed Tools",
    "add_tool": "Add Tool",
    "tools_config": "Tools Configuration",
    "tool_name": "Tool Name",
    "method": "Method",
    "endpoint": "Endpoint",
    "mcp_server_config": "MCP Server Configuration",
    "mcp_type": "MCP Type",
    "command": "Command",
    "args": "Arguments",
    "env_variables": "Environment Variables",
    "url": "URL",
    "http_method": "HTTP Method",
    "add_server": "Add Server",
    "startup_policy": "Startup Policy",
    "policy_on_demand": "Connect on Demand",
    "policy_on_start": "Connect on Start",
    "preinstalled": "Pre-install on Gateway Start",
    "arguments_config": "Arguments Configuration",
    "argument_name": "Argument Name",
    "argument_position": "Position",
    "argument_type": "Type",
    "argument_required": "Required",
    "argument_description": "Description",
    "argument_default": "Default Value",
    "add_argument": "Add Argument",
    "position_body": "Request Body",
    "position_query": "Query Parameters",
    "position_path": "Path Parameters",
    "type_string": "String",
    "type_number": "Number",
    "type_boolean": "Boolean",
    "type_array": "Array",
    "type_object": "Object",
    "type_form_data": "Form Data",
    "enable_cors": "Enable CORS",
    "enable_auth": "Enable Authentication",
    "auth_mode": "Authentication Mode",
    "allow_origins": "Allowed Origins",
    "allow_methods": "Allowed Methods",
    "allow_headers": "Allowed Headers",
    "expose_headers": "Exposed Headers",
    "credentials": "Allow Credentials",
    "http_proxy": "HTTP Proxy",
    "mcp_proxy": "MCP Proxy",
    "add_router": "Add Router",
    "add_mcp_server": "Add MCP Server",
    "server_description": "Server Description",
    "router_prefix": "Router Prefix",
    "router_server": "Router Server",
    "mcp_server_name": "MCP Server Name",
    "mcp_server_type": "MCP Server Type",
    "mcp_server_command": "MCP Server Command",
    "mcp_server_args": "MCP Server Arguments",
    "mcp_server_env": "MCP Server Environment Variables",
    "mcp_server_url": "MCP Server URL",
    "add_env_variable": "Add Environment Variable",
    "env_key": "Environment Variable Key",
    "env_value": "Environment Variable Value",
    "add_header": "Add Header",
    "header_key": "Header Key",
    "header_value": "Header Value",
    "tool_description": "Tool Description",
    "tool_method": "Tool Method",
    "tool_headers": "Tool Headers",
    "tool_request_body": "Tool Request Body",
    "tool_response_body": "Tool Response Body",
    "add_tool_success": "Tool added successfully",
    "add_tool_failed": "Failed to add tool",
    "add_server_success": "Server added successfully",
    "add_server_failed": "Failed to add server",
    "add_router_success": "Router added successfully",
    "add_router_failed": "Failed to add router",
    "add_mcp_server_success": "MCP server added successfully",
    "add_mcp_server_failed": "Failed to add MCP server",
    "add_env_variable_success": "Environment variable added successfully",
    "add_env_variable_failed": "Failed to add environment variable",
    "add_header_success": "Header added successfully",
    "add_header_failed": "Failed to add header",
    "remove_tool_success": "Tool removed successfully",
    "remove_tool_failed": "Failed to remove tool",
    "remove_server_success": "Server removed successfully",
    "remove_server_failed": "Failed to remove server",
    "remove_router_success": "Router removed successfully",
    "remove_router_failed": "Failed to remove router",
    "remove_mcp_server_success": "MCP server removed successfully",
    "remove_mcp_server_failed": "Failed to remove MCP server",
    "remove_env_variable_success": "Environment variable removed successfully",
    "remove_env_variable_failed": "Failed to remove environment variable",
    "remove_header_success": "Header removed successfully",
    "remove_header_failed": "Failed to remove header",
    "update_tool_success": "Tool updated successfully",
    "update_tool_failed": "Failed to update tool",
    "update_server_success": "Server updated successfully",
    "update_server_failed": "Failed to update server",
    "update_router_success": "Router updated successfully",
    "update_router_failed": "Failed to update router",
    "update_mcp_server_success": "MCP server updated successfully",
    "update_mcp_server_failed": "Failed to update MCP server",
    "update_env_variable_success": "Environment variable updated successfully",
    "update_env_variable_failed": "Failed to update environment variable",
    "update_header_success": "Header updated successfully",
    "update_header_failed": "Failed to update header",
    "http_servers": "HTTP Servers",
    "mcp_servers": "MCP Servers",
    "servers": "Servers",
    "routers": "Routers",
    "tools": "Tools",
    "request_body": "Request Body",
    "response_body": "Response Body",
    "request_body_placeholder": "Example: {\"uid\": \"{{.Args.uid}}\"}",
    "response_body_placeholder": "Example: {{.Response.Body}}",
    "origin_placeholder": "Example: https://example.com or *",
    "header_placeholder": "Example: Content-Type",
    "expose_header_placeholder": "Example: Content-Length",
    "sse_url": "SSE URL",
    "streamable_http_url": "Streamable HTTP URL",
    "tenant_name": "Tenant",
    "confirm_delete": "Are you sure you want to delete gateway '{{name}}'?",
    "remove_argument": "Remove Argument",
    "remove_tool": "Remove Tool",
    "tools_already_all_added": "Tools already all added",
    "tools_none_available": "No tools available",
    "remove_server": "Remove Server",
    "add_origin": "Add Origin",
    "add_method": "Add Method",
    "add_header_custom": "Add Header",
    "add_expose_header": "Add Expose Header",
    "url_access_note": "When using AllInOne, you have two options: access through Nginx to MCP Gateway, or directly access MCP Gateway using your configured modifier (update the base URL if different)",
    "direct_to_mcp_gateway": "Direct to MCP Gateway:",
    "server_name_limit": "{{count}}/{{max}} characters",
    "name_length_error": "Name length cannot exceed 50 characters",
    "sse_prefix": "SSE Prefix",
    "sse_prefix_placeholder": "Enter SSE prefix",
    "prompt": "Prompt",
    "prompts": "Prompts",
    "prompt_name": "Prompt Name",
    "arguments": "Arguments",
    "required": "Required",
    "remove_prompt": "Remove Prompt",
    "add_prompt": "Add Prompt",
    "prompt_response": "Prompt Response",
    "add_prompt_response": "Add Response",
    "prompt_response_role": "Role",
    "prompt_response_type": "Content Type",
    "prompt_response_text": "Text",
    "remove_prompt_response": "Remove Response",
    "array_items_config": "Array Items Configuration",
    "object_properties_config": "Object Properties Configuration",
    "array_item_type": "Array Item Type",
    "object_properties": "Object Properties",
    "add_property": "Add Property",
    "property_name": "Property Name",
    "property_type": "Property Type",
    "property_description": "Property Description",
    "header_name_placeholder": "Enter header name",
    "header_value_placeholder": "Enter header value",
    "select_file": "Select File",
    "prefix_placeholder": "Enter prefix (optional)",
    "upload_openapi_file": "Upload OpenAPI File",
    "drag_drop_openapi": "Drag and drop your OpenAPI specification",
    "or_click_to_browse": "or click to browse files",
    "import_openapi_description": "Import OpenAPI specification to create MCP server configuration"
  },
  "chat": {
    "title": "Chat",
    "send": "Send",
    "clear": "Clear",
    "stop": "Stop",
    "retry": "Retry",
    "thinking": "Thinking...",
    "generating": "Generating...",
    "welcome_message": "Hello! I'm MCP Assistant. How can I help you?",
    "error_message": "An error occurred. Please try again.",
    "empty_message": "Please enter a message",
    "tool_error": "Error loading tool: {{name}}",
    "tool_loading": "Loading tool: {{name}}",
    "tool_loaded": "Tool loaded: {{name}}",
    "tool_unloaded": "Tool unloaded: {{name}}",
    "tool_not_found": "Tool not found: {{name}}",
    "tool_already_loaded": "Tool already loaded: {{name}}",
    "tool_already_unloaded": "Tool already unloaded: {{name}}",
    "tool_load_failed": "Failed to load tool: {{name}}",
    "tool_unload_failed": "Failed to unload tool: {{name}}",
    "tool_load_success": "Tool loaded successfully: {{name}}",
    "tool_unload_success": "Tool unloaded successfully: {{name}}",
    "tool_load_error": "Error loading tool: {{name}}",
    "tool_unload_error": "Error unloading tool: {{name}}",
    "tool_load_timeout": "Timeout loading tool: {{name}}",
    "tool_unload_timeout": "Timeout unloading tool: {{name}}",
    "tool_load_cancelled": "Tool loading cancelled: {{name}}",
    "tool_unload_cancelled": "Tool unloading cancelled: {{name}}",
    "tool_load_retry": "Retrying tool load: {{name}}",
    "tool_unload_retry": "Retrying tool unload: {{name}}",
    "tool_load_max_retries": "Maximum retries reached for tool: {{name}}",
    "tool_unload_max_retries": "Maximum retries reached for tool: {{name}}",
    "tool_load_network_error": "Network error loading tool: {{name}}",
    "tool_unload_network_error": "Network error unloading tool: {{name}}",
    "tool_load_server_error": "Server error loading tool: {{name}}",
    "tool_unload_server_error": "Server error unloading tool: {{name}}",
    "tool_load_client_error": "Client error loading tool: {{name}}",
    "tool_unload_client_error": "Client error unloading tool: {{name}}",
    "tool_load_unknown_error": "Unknown error loading tool: {{name}}",
    "tool_unload_unknown_error": "Unknown error unloading tool: {{name}}",
    "message_placeholder": "Type your message...",
    "expand_history": "Expand chat history",
    "collapse_history": "Collapse chat history",
    "expand_tools": "Expand tools area",
    "collapse_tools": "Collapse tools area",
    "available_tools": "Available Tools",
    "server_tools": "Server Tools",
    "you": "You",
    "arguments": "Arguments",
    "result": "Result",
    "mcpServers": "MCP Servers",
    "selectMCPServers": "Select MCP Servers",
    "typeMessage": "Type your message...",
    "run_tool": "Run Tool",
    "execution_completed": "Execution Completed",
    "tool_call_success": "Tool call successful: {{result}}",
    "reasoning_process": "Reasoning Process",
    "tool_completed": "Completed",
    "tool_pending": "Pending",
    "notification_received": "Notification from {{server}}: {{message}}",
    "new_chat": "New Chat",
    "no_history": "No chat history",
    "untitled": "Untitled Chat",
    "resize_history": "Resize chat history",
    "rename": "Rename",
    "experimentalWarning": {
      "title": "Experimental Feature",
      "description": "This LLM chat feature is experimental and may not be fully compatible with all providers.",
      "reportIssue": "Report Issue",
      "or": " or ",
      "submitPR": "Submit PR",
      "helpImprove": " to help improve compatibility."
    },
    "delete": "Delete",
    "delete_session": "Delete Chat Session",
    "delete_session_confirm": "Are you sure you want to delete this chat session? This action cannot be undone.",
    "rename_title": "Rename Chat Session",
    "delete_title": "Delete Chat Session",
    "delete_confirm": "Are you sure you want to delete this chat session?",
    "rename_success": "Chat session renamed successfully",
    "rename_failed": "Failed to rename chat session",
    "delete_success": "Chat session deleted successfully",
    "delete_failed": "Failed to delete chat session",
    "rename_session": "Rename Chat Session",
    "session_title_placeholder": "Enter new session title",
    "systemPrompt": "System Prompt",
    "systemPromptPlaceholder": "Set a custom system prompt for the assistant (optional)",
    "editSystemPrompt": "Edit System Prompt",
    "authTokenPlaceholder": "Enter authentication token",
    "authToken": "Authentication Token"
  },
  "users": {
    "title": "User Management",
    "add": "Add User",
    "edit": "Edit User",
    "delete": "Delete User",
    "delete_title": "Delete User",
    "username": "Username",
    "password": "Password",
    "role": "Role",
    "status": "Status",
    "created_at": "Created At",
    "actions": "Actions",
    "role_admin": "Administrator",
    "role_normal": "Normal User",
    "status_enabled": "Enabled",
    "status_disabled": "Disabled",
    "password_placeholder": "Leave empty to keep current password",
    "confirm_delete": "Are you sure to delete this user?",
    "add_success": "User added successfully",
    "edit_success": "User updated successfully",
    "delete_success": "User deleted successfully",
    "add_failed": "Failed to add user",
    "edit_failed": "Failed to update user",
    "delete_failed": "Failed to delete user",
    "enable_success": "User enabled successfully",
    "disable_success": "User disabled successfully",
    "enable_failed": "Failed to enable user",
    "disable_failed": "Failed to disable user",
    "select_tenants": "Select Tenants",
    "select_tenant": "Select Tenant",
    "search_tenants": "Search tenants...",
    "update_tenants_success": "User tenant associations updated successfully",
    "update_tenants_failed": "Failed to update user tenant associations",
    "no_tenants": "No associated tenants",
    "manage_tenants": "Manage Tenants",
    "tenants": "Associated Tenants"
  },
  "tenants": {
    "title": "Tenant Management",
    "add": "Add Tenant",
    "edit": "Edit Tenant",
    "delete": "Delete Tenant",
    "delete_title": "Delete Tenant",
    "name": "Name",
    "name_placeholder": "Only English and numbers allowed",
    "prefix": "Prefix",
    "prefix_placeholder": "e.g. /tenant1 or /tenant2/abc (root / not allowed)",
    "description": "Description",
    "status": "Status",
    "created_at": "Created At",
    "actions": "Actions",
    "status_enabled": "Enabled",
    "status_disabled": "Disabled",
    "confirm_delete": "Are you sure you want to delete this tenant?",
    "add_success": "Tenant added successfully",
    "edit_success": "Tenant updated successfully",
    "delete_success": "Tenant deleted successfully",
    "add_failed": "Failed to add tenant",
    "edit_failed": "Failed to update tenant",
    "delete_failed": "Failed to delete tenant",
    "enable_success": "Tenant enabled successfully",
    "disable_success": "Tenant disabled successfully",
    "enable_failed": "Failed to enable tenant",
    "disable_failed": "Failed to disable tenant",
    "no_description": "No description",
    "save": "Save",
    "prefix_conflict": "Prefix already exists, please use another one",
    "name_conflict": "Tenant name already exists, please use another name",
    "prefix_path_conflict": "Prefix path conflict, cannot add a sub-path or parent path of an existing prefix",
    "root_prefix_not_allowed": "Root level directory '/' is not allowed as prefix"
  },
  "errors": {
    "required": "This field is required",
    "invalid_email": "Invalid email address",
    "invalid_password": "Invalid password",
    "invalid_username": "Invalid username",
    "invalid_url": "Invalid URL",
    "network_error": "Network error, please check your connection",
    "server_error": "Server error, please try again later",
    "timeout": "Request timeout",
    "unknown": "Unknown error",
    "check_system_status": "Failed to check system status",
    "fetch_mcp_servers": "Failed to fetch MCP servers",
    "fetch_tools": "Failed to fetch tools: {{error}}",
    "invalid_yaml": "Invalid YAML format",
    "fetch_users": "Failed to fetch user list",
    "fetch_user": "Failed to fetch user details",
    "invalid_tool_name": "Invalid tool name format",
    "invalid_tool_arguments": "Invalid tool arguments format",
    "server_not_connected": "Server {{server}} is not connected",
    "tool_call_failed": "Tool call failed: {{error}}",
    "fetch_chat_history": "Failed to fetch chat history: {{error}}",
    "load_messages": "Failed to load messages: {{error}}",
    "mcp_server_error": "Error in MCP server {{server}}: {{error}}",
    "connect_mcp_server": "Failed to connect to MCP server {{server}}",
    "no_server_config": "No configuration found for server {{server}}",
    "terminate_session": "Failed to terminate session: {{error}}",
    "disconnect_failed": "Failed to disconnect: {{error}}",
    "websocket_disconnected": "WebSocket connection disconnected",
    "websocket_error": "WebSocket error: {{error}}",
    "invalid_openapi_file": "Please select a valid OpenAPI specification file",
    "import_openapi_success": "Successfully imported OpenAPI specification",
    "import_openapi_failed": "Failed to import OpenAPI specification",
    "get_tools_failed": "Failed to get tools list: {{error}}",
    "call_tool_failed": "Failed to call tool {{toolName}}: {{error}}",
    "fetch_mcp_server": "Failed to fetch MCP server",
    "create_mcp_server": "Failed to create MCP server",
    "update_mcp_server": "Failed to update MCP server configuration",
    "delete_mcp_server": "Failed to delete MCP server",
    "sync_mcp_server": "Failed to sync MCP server",
    "fetch_chat_messages": "Failed to fetch chat messages",
    "fetch_chat_sessions": "Failed to fetch chat sessions",
    "fetch_tenants": "Failed to fetch tenant list",
    "fetch_tenant": "Failed to fetch tenant details",
    "create_tenant": "Failed to create tenant",
    "update_tenant": "Failed to update tenant",
    "delete_tenant": "Failed to delete tenant",
    "fetch_authorized_tenants": "Failed to fetch authorized tenants",
    "generic": "An error occurred. Please try again.",
    "not_found": "The requested resource was not found.",
    "unauthorized": "You are not authorized to perform this action.",
    "forbidden": "You do not have permission to access this resource.",
    "internal_server": "Internal server error. Please try again later.",
    "bad_request": "Bad request. Please check your input.",
    "conflict": "Conflict with existing resource.",
    "validation": "Validation error. Please check your input.",
    "parse_config": "Failed to parse configuration for {{name}}.",
    "namespace_permission_error": "User does not have permission to configure namespace",
    "tenant_permission_error": "User does not have permission to configure this tenant",
    "router_prefix_error": "Router prefix must start with tenant prefix, and cannot be empty",
    "tenant_required": "Tenant field is required",
    "tenant_not_found": "Tenant with the specified prefix does not exist",
    "validate_router_prefix_failed": "Failed to validate router prefix",
    "export_mcp_server": "Failed to export MCP server",
    "set_active_version": "Failed to set version as active",
    "llm_request_failed": "LLM request failed: {{error}}",
    "save_chat_message": "Failed to save chat message"
  },
  "mcp": {
    "configVersions": {
      "title": "Configuration Versions",
      "name": "Name",
      "tenant": "Tenant",
      "version": "Version",
      "created_by": "Created By",
      "created_at": "Created At",
      "action_type": "Action Type",
      "action_types": {
        "create": "Create",
        "update": "Update",
        "delete": "Delete",
        "revert": "Revert"
      },
      "active": "Active",
      "inactive": "Inactive",
      "actions": "Actions",
      "compare": "Compare",
      "compare_versions": "Compare Versions",
      "compare_with_previous": "Compare with Previous",
      "compare_with_latest": "Compare with Latest",
      "rollback": "Rollback to this version",
      "no_previous_version": "No previous version available for comparison",
      "no_latest_version": "No latest version available for comparison",
      "show_all": "Show All",
      "show_diff_only": "Show Diff Only",
      "select_config": "Select Config",
      "select_tenant": "Select Tenant",
      "fetch_names_error": "Failed to fetch config names",
      "fetch_error": "Failed to fetch versions",
      "set_active_success": "Version set as active successfully",
      "set_active_error": "Failed to set version as active"
    }
  },
  "llm": {
    "title": "LLM Providers",
    "description": "Configure language model providers",
    "management": "Provider Management",
    "providers": "Language Model Providers",
    "searchProviders": "Search providers...",
    "searchProvidersPlaceholder": "Search providers...",
    "searchModels": "Search models...",
    "addProvider": "Add Provider",
    "addFirstProvider": "Add Your First Provider",
    "noProviders": "No providers configured yet",
    "enabledCount": "{{count}} enabled",
    "enabled": "Enabled",
    "disabled": "Disabled",
    "custom": "Custom",
    "configuration": "Configuration",
    "models": "Models",
    "configure": "Configure",
    "test": "Test Connection",
    "testing": "Testing...",
    "testSuccess": "Connection successful",
    "testFailed": "Connection failed: {{error}}",
    "testError": "Test error: {{error}}",
    "connected": "Connected",
    "export": "Export Config",
    "import": "Import Config",
    "reset": "Reset to Default",
    "resetTooltip": "Reset all providers to default configuration",
    "resetConfirm": "Are you sure you want to reset all providers to default configuration? This will remove all custom settings.",
    "resetSuccess": "Successfully reset to default configuration",
    "importSuccess": "Configuration imported successfully",
    "importError": "Import failed: {{error}}",
    "addSuccess": "Provider '{{name}}' added successfully",
    "updateSuccess": "Provider configuration updated successfully",
    "deleteSuccess": "Provider '{{name}}' deleted successfully",
    "deleteConfirm": "Are you sure you want to delete provider '{{name}}'? This action cannot be undone.",
    "selectProvider": "Select a Provider",
    "customProvider": "Custom Provider",
    "addCustomProvider": "Add Custom Provider",
    "customProviderDesc": "Configure a custom OpenAI-compatible API provider",
    "providerName": "Provider Name",
    "providerNamePlaceholder": "Enter provider name",
    "nameRequired": "Provider name is required",
    "descriptionPlaceholder": "Enter provider description (optional)",
    "providerConfigNote": "You can configure the API key and other settings after adding the provider.",
    "configureProvider": "Configure {{name}}",
    "apiKey": "API Key",
    "apiKeyPlaceholder": "Enter your API key",
    "baseURL": "Base URL",
    "baseURLPlaceholder": "https://api.openai.com/v1",
    "organization": "Organization",
    "organizationPlaceholder": "Enter organization ID (optional)",
    "fetchOnClient": "Client-side Requests",
    "fetchOnClientDesc": "Make API calls directly from the browser instead of through the server",
    "clientMode": "Client Mode",
    "customEndpoint": "Custom Endpoint",
    "modelParameters": "Model Parameters",
    "temperature": "Temperature",
    "topP": "Top-p",
    "maxTokens": "Max Tokens",
    "maxOutput": "Max Output",
    "timeout": "Timeout",
    "contextWindow": "Context Window",
    "website": "Official Website",
    "documentation": "Documentation",
    "availableModels": "Available Models",
    "enabledModels": "Enabled Models",
    "noModels": "No models available",
    "modelsCount": "{{count}} models",
    "add": "Add",
    "noProviderSelected": "No provider selected",
    "selectProviderFirst": "Please select an LLM provider first",
    "openSettings": "Open Settings",
    "chatSettings": "Chat Settings",
    "manageLLMProviders": "Manage LLM Providers",
    "generating": "Generating response...",
    "selectProviderToConfig": "Select a provider to configure",
    "selectProviderToStart": "Select a provider from the left to start configuration",
    "modelsEnabled": "models enabled",
    "fetchModels": "Fetch Models",
    "fetching": "Fetching...",
    "addCustomModel": "Add Custom Model",
    "modelId": "Model ID",
    "modelName": "Model Name",
    "modelNameRequired": "Model name is required",
    "addModelSuccess": "Model added successfully",
    "deleteModelSuccess": "Model deleted successfully",
    "fetchModelsSuccess": "Fetched {{count}} new models",
    "fetchModelsFailed": "Failed to fetch models: {{error}}",
    "apiKeyRequired": "API key is required to fetch models",
    "updateFailed": "Failed to update configuration",
    "provider": "Provider",
    "model": "Model",
    "selectModel": "Select Model",
    "selectProviderAndModel": "Please select provider and model first",
    "noModelsAvailable": "No models available",
    "useClientFetchToLoadModels": "Use 'Fetch Models' to load available models",
    "tryOtherSearchTerms": "Try other search terms",
    "providerDescriptions": {
       "ai21": "AI21 Labs builds foundational models and AI systems for enterprises, accelerating the adoption of generative AI in production.",
        "ai360": "360 AI is a platform by 360 Company offering advanced NLP models like 360GPT2 Pro, 360GPT Pro, 360GPT Turbo, and 360GPT Turbo Responsibility 8K. These models combine large-scale parameters and multimodal capabilities, widely used in text generation, semantic understanding, dialogue systems, and code generation. Flexible pricing meets diverse user needs and supports developer integration, driving innovation in intelligent applications.",
        "anthropic": "Anthropic is an AI research and development company providing advanced language models such as Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Opus, and Claude 3 Haiku. These models balance intelligence, speed, and cost, suitable for enterprise workloads and fast-response applications. Claude 3.5 Sonnet, the latest, excels in evaluations while maintaining high cost-effectiveness.",
        "azure": "Azure offers advanced AI models including GPT-3.5 and the latest GPT-4 series, supporting various data types and complex tasks, focusing on secure, reliable, and sustainable AI solutions.",
        "azureai": "Azure offers advanced AI models including GPT-3.5 and the latest GPT-4 series, supporting various data types and complex tasks, focusing on secure, reliable, and sustainable AI solutions.",
        "baichuan": "Baichuan Intelligence focuses on large AI model R&D. Its models excel in Chinese knowledge, long text processing, and creative generation, outperforming foreign mainstream models. Baichuan also leads in multimodal capabilities and offers models like Baichuan 4, Baichuan 3 Turbo, and Baichuan 3 Turbo 128k for different scenarios.",
        "bedrock": "Bedrock by AWS provides advanced AI language and vision models for enterprises, including Anthropic's Claude series and Meta's Llama 3.1 series. It supports text generation, dialogue, image processing, and more for various business needs.",
        "cloudflare": "Run serverless GPU-powered machine learning models on Cloudflare's global network.",
        "cohere": "Cohere delivers cutting-edge multilingual models, advanced retrieval, and enterprise-ready AI workspaces—all in a secure platform.",
        "deepseek": "DeepSeek is an AI research and application company. Its latest DeepSeek-V3 model outperforms open-source models like Qwen2.5-72B and Llama-3.1-405B, and matches leading closed-source models like GPT-4o and Claude-3.5-Sonnet.",
        "fireworksai": "Fireworks AI is a leading provider of advanced language models, specializing in function calling and multimodal processing. The latest Firefunction V2, based on Llama-3, is optimized for function calls, dialogue, and instruction following. FireLLaVA-13B supports image and text input. Other notable models include Llama and Mixtral series, offering efficient multilingual instruction following and generation.",
        "giteeai": "Gitee AI's Serverless API offers out-of-the-box large model inference API services for AI developers.",
        "github": "With GitHub models, developers can become AI engineers and build with industry-leading AI models.",
        "google": "Google's Gemini series is its most advanced, general-purpose AI model, designed for multimodal tasks—text, code, image, audio, and video—by Google DeepMind. It is efficient and widely applicable from data centers to mobile devices.",
        "groq": "Groq's LPU inference engine excels in independent LLM benchmarks, redefining AI solution standards with its speed and efficiency. Groq represents instant inference speed and strong cloud performance.",
        "higress": "Higress is a cloud-native API gateway developed by Alibaba to address long-connection and load balancing issues for gRPC/Dubbo.",
        "huggingface": "HuggingFace Inference API provides fast, free access to thousands of models for various tasks, enabling rapid prototyping and ML experimentation.",
        "hunyuan": "Developed by Tencent, this large language model excels in Chinese content creation, logical reasoning in complex contexts, and reliable task execution.",
        "infiniai": "Provides high-performance, easy-to-use, and secure large model services for developers, covering the full process from model development to deployment.",
        "internlm": "An open-source organization focused on large model research and toolchains, providing efficient, user-friendly platforms for AI developers.",
        "jina": "Jina AI, founded in 2020, is a leading search AI company. Its platform includes vector models, rerankers, and small LLMs for building reliable generative AI and multimodal search applications.",
        "lmstudio": "LM Studio is a desktop app for developing and experimenting with LLMs on your computer.",
        "minimax": "MiniMax, founded in 2021, is an AGI company developing multimodal large models, including trillion-parameter MoE text, speech, and image models, and applications like Hailuo AI.",
        "mistral": "Mistral offers advanced general, specialized, and research models for complex reasoning, multilingual tasks, and code generation, with function calling interfaces for custom integration.",
        "moonshot": "Moonshot, by Beijing Moonshot AI, is an open platform offering various NLP models for content creation, research, recommendation, and medical diagnosis, supporting long text and complex generation.",
        "novita": "Novita AI is a platform providing various large language models and AI image generation API services, flexible, reliable, and cost-effective. It supports the latest open-source models like Llama3 and Mistral, offering comprehensive, user-friendly, and auto-scaling API solutions for generative AI application development, suitable for the rapid growth of AI startups.",
        "nvidia": "NVIDIA NIM™ provides containers for self-hosted GPU-accelerated inference microservices, supporting deployment of pre-trained and custom AI models on cloud, data center, RTX™ AI personal computers, and workstations.",
        "ollama": "Ollama offers models covering code generation, mathematical operations, multilingual processing, and dialogue interaction, supporting diverse enterprise-level and localized deployment needs.",
        "openai": "OpenAI is a global leader in AI research, with models like the GPT series pushing the frontiers of natural language processing. OpenAI aims to transform multiple industries through innovative and efficient AI solutions, with products widely used in research, business, and innovative applications.",
        "openrouter": "OpenRouter is a service platform providing interfaces to various cutting-edge large models, supporting OpenAI, Anthropic, LLaMA, and more, suitable for diverse development and application needs. Users can flexibly choose the optimal model and price according to their needs, enhancing their AI experience.",
        "perplexity": "Perplexity is a leading provider of dialogue generation models, offering various advanced Llama 3.1 models for online and offline applications, especially suitable for complex natural language processing tasks.",
        "ppio": "PPIO provides stable and cost-effective open-source model API services, supporting the full range of DeepSeek, Llama, Qwen, and other industry-leading large models.",
        "qiniu": "Qiniu, as an established cloud service provider, offers stable and cost-effective real-time and batch AI inference services, easy to use.",
        "qwen": "Qwen is an ultra-large-scale language model independently developed by Alibaba Cloud, with strong natural language understanding and generation capabilities. It can answer questions, create text content, express opinions, and write code, among other functions, playing a role in various fields.",
        "sambanova": "SambaNova Cloud enables developers to easily use the best open-source models and enjoy the fastest inference speeds.",
        "search1api": "Search1API provides access to the DeepSeek series models, which can connect to the internet as needed, including standard and fast versions, supporting a variety of parameter scale model selections.",
        "sensenova": "SenseTime's daily renewal, relying on the strong基础支撑 of SenseTime's大装置, provides efficient and easy-to-use全栈大模型服务.",
        "siliconcloud": "SiliconCloud, high cost-performance GenAI cloud service based on excellent open-source基础模型",
        "spark": "iFLYTEK's Xinghuo large model provides powerful AI capabilities in multiple fields and languages, using advanced natural language processing technology to build innovative applications suitable for intelligent hardware, smart medical care, smart finance, and other vertical scenes.",
        "stepfun": "StepFun's StarrySky large model has industry-leading multimodal and complex reasoning capabilities, supporting ultra-long text understanding and powerful autonomous scheduling search engine functions.",
        "taichu": "The new generation of multimodal large model launched by the Institute of Automation, Chinese Academy of Sciences and Wuhan Artificial Intelligence Research Institute supports comprehensive Q&A tasks such as multi-turn Q&A, text creation, image generation, 3D understanding, and signal analysis, with stronger cognitive, understanding, and creation capabilities, bringing a brand-new interactive experience.",
        "tencentcloud": "Knowledge Engine Atomic Power, based on the Knowledge Engine, provides a full-link capability for knowledge Q&A, offering flexible assembly and development of model applications for enterprises and developers. You can assemble your exclusive model service through multiple atomic capabilities, calling document parsing, splitting, embedding, multi-turn rewriting, and other services for customization of enterprise-specific AI业务.",
        "togetherai": "Together AI is committed to achieving leading performance through innovative AI models, providing extensive customization capabilities, including fast expansion support and intuitive deployment processes, to meet various enterprise needs.",
        "upstage": "Upstage focuses on developing AI models for various business needs, including Solar LLM and Document AI, aiming to实现人造通用智能 (AGI) for work. Create simple dialogue agents with Chat API, supporting function calling, translation, embedding, and specific domain applications.",
        "vertexai": "Google's Gemini series is its most advanced, general-purpose AI model, designed for multimodal tasks—text, code, image, audio, and video—by Google DeepMind. It is efficient and widely applicable from data centers to mobile devices.",
        "vllm": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
        "volcengine": "ByteDance's developed model service platform provides feature-rich, secure, and competitively priced model calling services, as well as end-to-end functions such as model data,精调,推理,评测,全面保障 your AI application development and landing.",
        "wenxin": "Enterprise-level one-stop large model and AI native application development and service platform, providing the most comprehensive and easy-to-use generative artificial intelligence model development and application development全过程工具链",
        "xai": "xAI is a company dedicated to building artificial intelligence to accelerate human scientific discovery. Our mission is to promote our共同理解 of the universe.",
        "xinference": "Xorbits Inference (Xinference) is an open-source platform for simplifying the运行和集成 of various AI models. With Xinference, you can run inferences using any open-source LLM, embedding model, and multimodal model in cloud or local environments, and create powerful AI applications.",
        "zeroone": "Zero One万物 is committed to promoting a人本的AI 2.0技术革命, aiming to create巨大经济和社会价值 through large language models, and开创新的AI生态与商业模式.",
        "zhipu": "Zhiyun AI provides an open platform for multimodal and language models, supporting a wide range of AI application scenarios, including text processing, image understanding, and programming assistance."
      },
    "modelDescriptions": {
      "openai": {
        "gpt-4.1": "GPT-4.1 is our flagship model for complex tasks. It is particularly well-suited for solving cross-domain problems.",
        "gpt-4.1-mini": "GPT-4.1 mini provides a balance between intelligence, speed, and cost, making it an attractive model for many use cases.",
        "gpt-4.1-nano": "GPT-4.1 nano is the fastest, most cost-effective GPT-4.1 model.",
        "chatgpt-4o-latest": "ChatGPT-4o is a dynamic model that updates in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
        "o3-mini": "o3-mini provides high intelligence within the same cost and latency targets as o1-mini.",
        "o3": "o3 is OpenAI's latest reasoning model with enhanced logical thinking capabilities, particularly excelling at complex problem-solving in mathematics, science, and programming.",
        "o4-mini": "o4-mini is the newest compact model in the GPT-4 series, offering improved efficiency and performance for a wide range of tasks at a lower cost.",
        "o1-mini": "o1-mini is a fast, economical and efficient inference model designed for programming, mathematics and scientific application scenarios. This model has 128K context and knowledge cutoff date of October 2023.",
        "o1": "o1 is OpenAI's new inference model, supporting image and text input with text output, suitable for complex tasks requiring broad general knowledge. This model has 200K context and knowledge cutoff date of October 2023.",
        "o1-preview": "o1 is OpenAI's new inference model, suitable for complex tasks requiring broad general knowledge. This model features 128K context and a knowledge cutoff date of October 2023.",
        "gpt-4.5-preview": "The research preview version of GPT-4.5, which is our largest and most capable GPT model to date. It has broad world knowledge and can better understand user intent, making it excel at creative tasks and autonomous planning. GPT-4.5 accepts text and image inputs and generates text outputs (including structured outputs). It supports key developer features like function calling, batch API, and streaming output. GPT-4.5 particularly excels at tasks requiring creativity, open-ended thinking, and dialogue (like writing, learning, or exploring new ideas). Knowledge cutoff: October 2023.",
        "gpt-4o-mini": "GPT-4o mini is OpenAI's latest model after GPT-4 Omni, supporting image-text input and outputting text. As their most advanced small model, it is much more affordable than other recent frontier models, and over 60% cheaper than GPT-3.5 Turbo. It maintains strong performance in chat, text and vision, and supports text and vision in API.",
        "gpt-4o-2024-11-20": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
        "gpt-4o": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
        "gpt-4o-2024-05-13": "ChatGPT-4o is a dynamic model that updates in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
        "gpt-4o-audio-preview": "GPT-4o real-time version, supporting audio and text real-time input and output",
        "gpt-4-turbo": "GPT-4 Turbo with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.",
        "gpt-4-turbo-2024-04-09": "GPT-4 Turbo with Vision. The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.",
        "gpt-4-turbo-preview": "GPT-4 Turbo preview model intended to reduce cases of \"laziness\" where the model doesn't complete a task.",
        "gpt-4-0125-preview": "GPT-4 Turbo preview model. The latest GPT-4 model intended to reduce cases of \"laziness\" where the model doesn't complete a task.",
        "gpt-4-1106-preview": "GPT-4 Turbo preview model. The latest GPT-4 model intended to reduce cases of \"laziness\" where the model doesn't complete a task.",
        "gpt-4": "More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.",
        "gpt-4-0613": "Snapshot of gpt-4 from June 13th 2023 with improved function calling support.",
        "gpt-4-32k": "Same capabilities as the standard gpt-4 mode but with 4x the context length.",
        "gpt-3.5-turbo": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-0125": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-1106": "GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more.",
        "gpt-3.5-turbo-instruct": "Similar capabilities to text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions."
      },
      "Skylark2-pro-4k": "The second generation model of Skylark, the Skylark2-pro model has higher model accuracy, suitable for more complex text generation scenarios such as professional domain document generation, novel creation, high-quality translation, etc., with a context window length of 4k",
      "Qwen/QVQ-72B-Preview": "QVQ-72B-Preview is a research model developed by the Qwen team focused on visual reasoning capabilities, with unique advantages in complex scene understanding and solving vision-related mathematical problems",
  	  "llama3.1:70b": "Llama 3.1 is a cutting-edge model launched by Meta, supporting up to 405B parameters, applicable for complex conversations, multilingual translation, and data analysis fields",
      "openai/gpt-4o": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support",
      "gemini-1.5-flash-exp-0827": "Gemini 1.5 Flash 0827 provides optimized multimodal processing capabilities, suitable for various complex task scenarios",
      "gemini-1.5-flash-002": "Gemini 1.5 Flash 002 is a high-efficiency multimodal model, supporting extensive application extensions",
      "gpt-3.5-turbo-0125": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
      "thudm/glm-4-32b:free": "GLM-4-32B-0414 is a 32B bilingual (Chinese-English) open-weight language model optimized for code generation, function calling, and agent-style tasks. It has been pre-trained on 15T high-quality and heavily curated data, and further refined using human preference alignment, rejection sampling, and reinforcement learning. This model performs excellently in complex reasoning, tool generation, and structured output tasks, achieving performance comparable to GPT-4o and DeepSeek-V3-0324 in multiple benchmark tests",
      "ministral-3b-latest": "Ministral 3B is Mistral's world-class edge model",
      "compound-beta-mini": "Compound-beta-mini is an AI system supported by models already available in GroqCloud that can intelligently and selectively use tools to answer user queries",
      "SenseChat-Turbo": "Suitable for quick Q&A and model fine-tuning scenarios",
      "glm-4v-plus": "GLM-4V-Plus has the ability to understand video content and multiple images, suitable for multimodal tasks",
      "step-1o-vision-32k": "This model has powerful image understanding capabilities. Compared to the step-1v series models, it has stronger visual performance",
      "gpt-4-vision-preview": "GPT-4 Vision Preview version, specifically designed for image analysis and processing tasks",
      "mistral-large-instruct": "Mistral-Large-Instruct-2407 is an advanced dense large language model (LLM) with 123 billion parameters, possessing state-of-the-art inference, knowledge, and coding capabilities",
      "gemini-1.5-flash-8b": "Gemini 1.5 Flash 8B is a high-efficiency multimodal model, supporting extensive application extensions",
      "google/gemini-2.0-flash-001": "Gemini 2.0 Flash provides next-generation features and improvements, including superior speed, native tool usage, multimodal generation, and 1M token context window",
      "gemma2": "Gemma 2 is a high-efficiency model launched by Google, covering various application scenarios from small applications to complex data processing",
      "x1": "Spark X1 model will be further upgraded, building upon its domestic leading foundation in mathematical tasks, achieving effects comparable to OpenAI o1 and DeepSeek R1 in reasoning, text generation, language understanding, and other general tasks",
      "ERNIE-Speed-128K": "Baidu's latest self-developed high-performance large language model released in 2024, with excellent general capabilities, suitable as a base model for fine-tuning to better handle specific scenario problems, while possessing optimal inference performance",
      "qwen2.5:0.5b": "Qwen2.5 is Alibaba's new generation of large language models, supporting diverse application needs with excellent performance",
      "Yi-34B-Chat": "Yi-1.5-34B maintains the excellent general language capabilities of the original series model while significantly improving mathematical logic and code capabilities through incremental training on 5 trillion high-quality tokens",
      "SenseCat-5-1202": "Based on V5.5's latest version, compared to the previous version, it shows significant improvements in Chinese-English basic capabilities, chat, scientific knowledge, literary knowledge, writing, mathematical logic, character control, and other dimensions",
      "hunyuan-lite-vision": "Latest 8B multimodal model with 2K context window, supporting multimodal dialogue, image object recognition, document table understanding, multimodal mathematics, etc., in Chinese and English scenarios, with evaluation indicators superior to 8B competitive models in multiple dimensions",
      "chatglm3-6b-base": "ChatGLM3-6b-base is the latest generation 6 billion parameter open-source base model of the ChatGLM series developed by Zhipu AI",
      "ernie-4.5-turbo-32k": "Wenxin 4.5 Turbo also shows significant enhancements in hallucination prevention, logical reasoning, and code capabilities. Compared to Wenxin 4.5, it's faster and more cost-effective. Text creation and knowledge Q&A capabilities have improved significantly. Output length and sentence delay have increased compared to ERNIE 4.5",
      "pixtral-large-latest": "Pixtral Large is an open-source multimodal model with 124 billion parameters, built on Mistral Large 2. This is our second model in the multimodal family, demonstrating state-of-the-art image understanding capabilities",
      "Qwen2.5-Coder-14B-Instruct": "Qwen2.5-Coder-14B-Instruct is a programming instruction model based on large-scale pre-training, with powerful code understanding and generation capabilities, capable of efficiently handling various programming tasks, particularly suitable for intelligent code writing, automation script generation, and programming problem solving",
      "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": "LLaMA 3.2 aims to handle tasks combining visual and textual data. It excels in image description and visual Q&A tasks, bridging the gap between language generation and visual reasoning",
      "gemini-2.0-flash-001": "Gemini 2.0 Flash provides next-generation features and improvements, including superior speed, native tool usage, multimodal generation, and 1M token context window",
      "01-ai/yi-1.5-9b-chat": "Zero One Everything, latest open-source fine-tuned model, 90 billion parameters, fine-tuning supports multiple dialogue scenarios, high-quality training data, aligned with human preferences",
      "meta-llama/Meta-Llama-3.1-405B-Instruct": "Llama 3.1 is a cutting-edge model launched by Meta, supporting up to 405B parameters, applicable for complex conversations, multilingual translation, and data analysis fields",
      "google/gemini-2.5-pro-preview-03-25": "Gemini 2.5 Pro is Google's most advanced AI model, designed specifically for advanced reasoning, coding, mathematics, and scientific tasks. It employs 'thinking' capabilities to process responses with higher accuracy and nuanced context handling. Gemini 2.5 Pro has achieved top-tier performance in multiple benchmarks, including ranking first on the LMArena leaderboard, reflecting superior human preference alignment and complex problem-solving abilities",
      "step-1-flash": "High-speed model, suitable for real-time dialogue",
      "360gpt-turbo-responsibility-8k": "360GPT Turbo Responsibility 8K emphasizes semantic safety and responsibility orientation, specifically designed for application scenarios with high content security requirements, ensuring accuracy and stability of user experience",
      "claude-3-sonnet-20240229": "Claude 3 Sonnet provides an ideal balance of intelligence and speed for enterprise workloads. It delivers maximum value at a lower price point, reliable and suitable for large-scale deployment",
      "meta-llama/Llama-3.3-70B-Instruct-Turbo": "Meta Llama 3.3 multilingual large language model (LLM) is a 70B (text-in text-out) pre-trained and instruction-tuned generative model. Llama 3.3 instruction-tuned pure text model has been optimized for multilingual dialogue use cases and outperforms many available open-source and closed chat models on common industry benchmarks",
      "cohere-command-r-plus": "Command R+ is a state-of-the-art RAG-optimized model designed to handle enterprise-grade workloads",
      "SenseNova-V6-Turbo": "Achieves native unification of image, text, and video capabilities, breaking through traditional multimodal separation limitations, leading comprehensively in multimodal basic capabilities, language basic capabilities, and other core dimensions, with both literary and practical applications, repeatedly ranking at domestic top-tier levels in multiple evaluations",
      "glm-4-9b-chat": "GLM-4-9B-Chat is the latest generation pre-trained model GLM-4-9B's human preference aligned version launched by Zhipu AI",
      "InternVL2.5-26B": "InternVL2.5-26B is a powerful visual language model that supports multimodal processing of images and text, capable of accurately identifying image content and generating relevant descriptions or answers",
      "qwen/qwen3-235b-a22b:free": "Qwen3-235B-A22B is a 235B parameter Mixture of Experts (MoE) model developed by Qwen, activating 22B parameters in each forward pass. It supports seamless switching between 'thinking' mode for complex reasoning, mathematics, and code tasks, and 'non-thinking' mode for general conversation efficiency. The model demonstrates powerful reasoning capabilities, multilingual support (100+ languages and dialects), advanced instruction following, and agent tool calling abilities. It natively processes 32K token context windows and extends to 131K tokens using YaRN-based extension",
      "qwen2.5-coder-instruct": "Qwen2.5-Coder is the latest code-specific large language model in the Qwen series (formerly CodeQwen)",
      "gemini-1.5-pro-002": "Gemini 1.5 Pro 002 is the latest production-ready model, providing higher quality output, with notable improvements particularly in mathematics, long context, and visual tasks",
      "qwen/qwen3-32b": "Qwen3 is a new generation of general-purpose thousand-question large model with significantly improved capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent, and multilingual support, and supporting thinking mode switching",
      "deepseek-r1-distill-llama-70b": "DeepSeek-R1-Distill-Llama-70B is DeepSeek-R1's distillation model based on Llama3.3-70B-Instruct",
      "openai/o1-preview": "o1 is OpenAI's new inference model, suitable for complex tasks requiring broad general knowledge. This model features 128K context and a knowledge cutoff date of October 2023",
      "Qwen/Qwen2.5-72B-Instruct": "Qwen2.5-72B-Instruct is one of the latest large language models released by Alibaba Cloud. This 72B model shows significant improvements in coding and mathematics capabilities. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows notable improvements in instruction following, understanding structured data, and generating structured output (especially JSON)",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": "DeepSeek-R1 distillation model, optimizing inference performance through reinforcement learning and cold-start data, refreshing task benchmarks for open-source models",
      "SenseNova-V6-Reasoner": "Balancing visual and language deep reasoning, achieving slow thinking and deep reasoning, showing complete thought chain processes",
      "deepseek-r1-online": "DeepSeek R1 full version with 671B parameters, supports real-time internet search, possessing stronger understanding and generation capabilities.",
      "meta.llama3-70b-instruct-v1:0": "Meta Llama 3 is an open large language model (LLM) for developers, researchers, and enterprises, aimed at helping them build, experiment, and responsibly scale their AI ideas. As part of the global community innovation infrastructure, it is particularly suitable for content creation, AI conversation, language understanding, research and development, and enterprise applications.",
      "command-light-nightly": "To shorten the time between major version releases, we launched the nightly version of the Command model. For the command-light series, this version is called command-light-nightly. Note that command-light-nightly is the newest, most experimental, and (potentially) unstable version. Nightly versions are updated regularly without prior notice, so they are not recommended for use in production environments.",
      "llama-3.1-8b-instant": "Llama 3.1 8B is a high-performance model providing fast text generation capabilities, particularly suitable for scenarios requiring large-scale efficiency and cost-effectiveness.",
      "360zhinao2-o1": "360zhinao2-o1 uses tree search to build thinking chains and introduces reflection mechanisms, trained with reinforcement learning, giving the model self-reflection and error correction capabilities.",
      "Pro/Qwen/Qwen2-7B-Instruct": "Qwen2-7B-Instruct is an instruction-tuned large language model in the Qwen2 series with 7B parameters. The model is based on the Transformer architecture and employs SwiGLU activation function, attention QKV bias, and group query attention. It can handle large-scale inputs. The model performs excellently in multiple benchmark tests including language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning, surpassing most open-source models and showing competitive performance comparable to proprietary models in certain tasks. Qwen2-7B-Instruct outperforms Qwen1.5-7B-Chat in various evaluations, demonstrating significant performance improvements",
      "qwen-coder-turbo-latest": "Tongyi Qianwen code model.",
      "llama-3.2-90b-vision-instruct": "Advanced image inference capabilities suitable for visual understanding agent applications.",
      "codeqwen": "CodeQwen1.5 is a large language model trained on massive code data, specifically designed to solve complex programming tasks.",
      "qwen/qwen3-235b-a22b": "Qwen3 is a new generation of general-purpose thousand-question large model with significantly improved capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual support, and supports thinking mode switching.",
      "internlm/internlm2_5-7b-chat": "InternLM2.5-7B-Chat is an open-source dialogue model developed based on the InternLM2 architecture. This 7B parameter model focuses on dialogue generation tasks and supports Chinese-English bilingual interaction. The model employs the latest training techniques aimed at providing fluent and intelligent conversation experiences. InternLM2.5-7B-Chat is suitable for various dialogue application scenarios, including but not limited to intelligent customer service and personal assistants",
      "togethercomputer/StripedHyena-Nous-7B": "StripedHyena Nous (7B) provides powerful computational capabilities through efficient strategies and model architecture.",
      "llama3.1": "Llama 3.1 is a cutting-edge model released by Meta, supporting up to 405B parameters, applicable for complex conversations, multilingual translation, and data analysis fields.",
      "command-r-03-2024": "Command R is an instruction-following dialogue model that demonstrates higher quality, more reliability, and longer context length compared to previous models. It can be used for complex workflows such as code generation, retrieval-augmented generation (RAG), tool use, and agency.",
      "ERNIE-4.0-Turbo-8K-Latest": "Baidu's self-developed flagship ultra-large language model, showing excellent comprehensive performance, widely applicable to complex task scenarios across various domains; supports automatic integration with Baidu search plugin, ensuring up-to-date question-answering information. Shows better performance compared to ERNIE 4.0",
      "yi-lightning-lite": "Lightweight version, recommended to use yi-lightning.",
      "qwen/qwen2.5-32b-instruct": "Qwen2.5-32B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 32B model shows significantly improved capabilities in coding and mathematics domains. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows notable improvements in instruction following, understanding structured data, and generating structured output (especially JSON).",
      "thudm/glm-4-9b:free": "GLM-4-9B-0414 is a 90 billion parameter language model in the GLM-4 series developed by THUDM. GLM-4-9B-0414 uses the same reinforcement learning and alignment strategies as its larger 32B counterpart for training, achieving high performance relative to its size, making it suitable for resource-constrained deployments that still require powerful language understanding and generation capabilities.",
      "qwen-turbo": "Tongyi Qianwen ultra-large language model, supporting input in different languages including Chinese and English.",
      "qwen2.5-instruct": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we have released multiple base language models and instruction-tuned language models, with parameter ranges from 5 billion to 72 billion.",
      "gpt-4o-realtime-preview": "GPT-4o real-time version, supporting audio and text real-time input and output",
      "open-mistral-7b": "Mistral 7B is a compact yet high-performance model excelling at batch processing and simple tasks like classification and text generation, with good reasoning capabilities.",
      "gemini-1.5-flash-001": "Gemini 1.5 Flash 001 is a high-efficiency multimodal model supporting extensive application extensions.",
      "moonshot-v1-auto": "Moonshot V1 Auto can select appropriate models based on the current context's token count",
      "thudm/glm-4-32b": "GLM-4-32B-0414 is a 32B bilingual (Chinese-English) open-weight language model optimized for code generation, function calling, and agent-style tasks. It has been pre-trained on 15T high-quality and heavy reasoning data, and further refined using human preference alignment, rejection sampling, and reinforcement learning. This model performs excellently in complex reasoning, tool generation, and structured output tasks, achieving performance comparable to GPT-4o and DeepSeek-V3-0324 in multiple benchmark tests.",
      "ernie-4.5-turbo-vl-32k": "The latest version of Wenxin multimodal large model, with significant improvements in image understanding, creation, translation, code, and other capabilities, first time supporting 32K context length, with notably reduced first token latency.",
      "thudm/glm-z1-rumination-32b": "GLM Z1 Rumination 32B is the 32B parameter deep reasoning model in the GLM-4-Z1 series, optimized for complex, open-ended tasks requiring long-term thinking. Built upon glm-4-32b-0414, it adds additional reinforcement learning stages and multi-stage alignment strategies, introducing \"rumination\" capabilities aimed at simulating extended cognitive processing. This includes iterative reasoning, multi-hop analysis, and tool-enhanced workflows such as search, retrieval, and citation-aware synthesis.\n\nThe model excels in research writing, comparative analysis, and complex question answering. It supports function calling for search and navigation primitives (`search`, `click`, `open`, `finish`), allowing it to be used in agent-like channels. Rumination behavior is controlled by multi-loop control with rule-based reward construction and delayed decision mechanisms, benchmarked against OpenAI's internal alignment stack and other deep research frameworks. This variant is suitable for scenarios requiring depth rather than speed.",
      "ai21-jamba-1.5-large": "A 198B parameter (34B active) multilingual model providing 56K long context window, function calling, structured output, and fact-based generation.",
      "Qwen/QwQ-32B-Preview": "Qwen QwQ is an experimental research model developed by the Qwen team, focusing on enhancing AI reasoning capabilities.",
      "openai/o4-mini-high": "o4-mini high inference grade version, optimized for fast and effective reasoning, showing extremely high efficiency and performance in coding and visual tasks.",
      "qwen-math-turbo-latest": "Tongyi Qianwen math model is a language model specifically designed for mathematical problem solving.",
      "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": "Qwen2.5-Coder-7B-Instruct is the latest version of Alibaba Cloud's code-specific large language model series. The model, based on Qwen2.5, has significantly improved code generation, reasoning, and correction capabilities through training on 55 billion tokens. It not only enhances coding abilities but also maintains advantages in mathematics and general capabilities. The model provides a more comprehensive foundation for practical applications such as code intelligence systems",
      "THUDM/GLM-Z1-32B-0414": "GLM-Z1-32B-0414 is a reasoning model with deep thinking capabilities. This model was developed based on GLM-4-32B-0414 through cold start and extended reinforcement learning, and underwent further training in mathematics, code, and logical tasks. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and complex problem-solving capabilities.",
      "mistralai/Mistral-7B-Instruct-v0.2": "Mistral (7B) Instruct v0.2 provides improved instruction handling capabilities and more accurate results.",
      "hunyuan-turbos-vision": "This model is suitable for image-text understanding scenarios, being the new generation visual language flagship model based on the latest Hunyuan turbos, focusing on image-text understanding related tasks, including image-based entity recognition, knowledge Q&A, document creation, photo problem solving, etc., showing comprehensive improvements compared to the previous generation model.",
      "c4ai-aya-expanse-32b": "Aya Expanse is a high-performance 32B multilingual model aiming to challenge monolingual model performance through innovations in instruction tuning, data selection, preference training, and model merging. It supports 23 languages.",
      "deepseek_r1_distill_qwen_32b": "DeepSeek-R1-Distill-Qwen-32B is a model obtained through knowledge distillation from Qwen2.5-32B. The model was fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating excellent performance in mathematics, programming, and reasoning across multiple domains.",
      "command-r7b-12-2024": "command-r7b-12-2024 is a small and efficient updated version released in December 2024. It excels in tasks requiring complex reasoning and multi-step processing, such as RAG, tool use, and agency.",
      "Qwen/Qwen2.5-VL-32B-Instruct": "Qwen2.5-VL-32B-Instruct is a multimodal large model launched by the Tongyi Qianwen team, part of the Qwen2.5-VL series. The model not only excels at recognizing common objects but can also analyze text, charts, icons, graphics, and layouts in images. It can serve as a visual intelligence agent, capable of reasoning and dynamically controlling tools, with the ability to use computers and phones. Additionally, this model can accurately locate objects in images and generate structured output for items like receipts and tables. Compared to the previous generation model Qwen2-VL, this version has been further improved in mathematics and problem-solving abilities through reinforcement learning, and its response style is more aligned with human preferences.",
      "claude-3-5-haiku-20241022": "Claude 3.5 Haiku is Anthropic's fastest next-generation model. Compared to Claude 3 Haiku, Claude 3.5 Haiku shows improvements across all technical capabilities and has surpassed the previous generation's largest model, Claude 3 Opus, in many intelligence benchmark tests.",
      "step-1v-8k": "Small visual model suitable for basic image-text tasks.",
      "databricks/dbrx-instruct": "DBRX Instruct provides reliable instruction processing capabilities, supporting various industry applications.",
      "qwen3-30b-a3b": "Qwen3 is a new generation of general-purpose thousand-question large model with significantly improved capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual support, and supports thinking mode switching.",
      "hunyuan-turbo-latest": "General experience optimization, including NLP understanding, text creation, casual conversation, knowledge Q&A, translation, domain expertise, etc.; enhanced anthropomorphization, optimized model emotional intelligence; improved ability to actively clarify intent ambiguity; enhanced handling of word analysis problems; improved quality and interactivity of creation; enhanced multi-round experience.",
      "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": "Meta's instruction-tuned image inference model with 900 billion parameters. This model has been optimized for visual recognition, image reasoning, image caption generation, and image-related common Q&A. It can understand visual data such as charts and diagrams, and bridges the gap between vision and language by generating textual descriptions of image details. Note: This model is currently being offered experimentally as a serverless model. For production environments, please note that Fireworks may discontinue deploying this model in the short term.",
      "codellama/CodeLlama-34b-Instruct-hf": "Code Llama is an LLM focused on code generation and discussion, combining broad programming language support, suitable for developer environments.",
      "Doubao-pro-256k": "The most effective mainstream model, suitable for handling complex tasks, with excellent results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports inference and fine-tuning with 256k context window.",
      "ERNIE-4.0-Turbo-8K-Preview": "Baidu's self-developed flagship ultra-large language model, showing excellent comprehensive performance, widely applicable to complex task scenarios across various domains; supports automatic integration with Baidu search plugin, ensuring up-to-date question-answering information. Shows better performance compared to ERNIE 4.0",
      "ernie-char-8k": "Baidu's self-developed vertical scenario large language model, suitable for game NPCs, customer service dialogue, conversational role-playing and other application scenarios, with clearer and more consistent character design style, stronger instruction following ability, and better reasoning performance.",
      "glm-z1-flash": "The GLM-Z1 series possesses strong complex reasoning capabilities, showing excellence in logical reasoning, mathematics, programming, and other fields. Maximum context length is 32K.",
      "openai/gpt-4.1": "GPT-4.1 is our flagship model for complex tasks. It is particularly well-suited for solving cross-domain problems.",
      "Doubao-vision-lite-32k": "The Doubao-vision model is a multimodal large model launched by Doubao, featuring powerful image understanding and reasoning capabilities, as well as accurate instruction understanding abilities. The model has demonstrated strong performance in tasks such as image text information extraction and image-based reasoning, capable of handling more complex and broader visual question-answering tasks.",
      "qwen-vl-plus-latest": "Enhanced version of the Qianwen large-scale visual language model. Significantly improves detail recognition and text recognition capabilities, supporting image resolution up to millions of pixels and images of any aspect ratio.",
      "qwen-omni-turbo-latest": "Qwen-Omni series models support input of multiple modalities including video, audio, images, and text, and can output both audio and text.",
      "ERNIE-3.5-8K-Preview": "Baidu's self-developed flagship large language model, covering massive Chinese-English corpus, with powerful general capabilities that can meet the demands of most dialogue Q&A, creative generation, and plugin application scenarios; supports automatic connection with Baidu search plugin, ensuring Q&A information timeliness.",
      "Qwen/Qwen3-8B": "Qwen3 is a new generation of Qianwen large model with significantly enhanced capabilities, achieving industry-leading performance in core abilities such as reasoning, general usage, Agent, and multilingual support, and supports switching between thinking modes.",
      "ERNIE-Speed-Pro-128K": "Baidu's latest self-developed high-performance large language model released in 2024, with excellent general capabilities, performing better than ERNIE Speed, suitable for fine-tuning as a base model to better handle specific scenario issues, while having optimal inference performance.",
      "glm-4-flashx": "GLM-4-FlashX is the enhanced version of Flash, featuring ultra-fast inference speed.",
      "gpt-4.1-mini": "GPT-4.1 mini provides a balance between intelligence, speed, and cost, making it an attractive model for many use cases.",
      "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": "DeepSeek-R1-Distill-Qwen-1.5B is a model obtained through knowledge distillation based on Qwen2.5-Math-1.5B. The model was fine-tuned using 800,000 high-quality samples generated by DeepSeek-R1, showing impressive performance across multiple benchmarks. As a lightweight model, it achieved 83.9% accuracy on MATH-500, 28.9% pass rate on AIME 2024, and a rating of 954 on CodeForces, demonstrating reasoning capabilities beyond its parameter scale.",
      "deepseek/deepseek-r1": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, featuring very strong reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, programming, and reasoning tasks.",
      "meta-llama/Llama-3-70b-chat-hf": "Llama 3 70B Instruct Reference is a powerful chat model that supports complex dialogue requirements.",
      "sonar": "A lightweight search-based contextual product, faster and more convenient than Sonar Pro.",
      "chatgpt-4o-latest": "ChatGPT-4o is a dynamic model that updates in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
      "meta.llama3-1-405b-instruct-v1:0": "Meta Llama 3.1 405B Instruct is the largest and most powerful model in the Llama 3.1 Instruct series, being a highly advanced dialogue inference and synthetic data generation model that can also serve as a foundation for professional continuous pre-training or fine-tuning in specific domains. The Llama 3.1 family of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in sizes of 8B, 70B, and 405B (text input/output). The Llama 3.1 instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and exceed many available open-source chat models in common industry benchmarks. Llama 3.1 is intended for commercial and research use in multiple languages. The instruction-tuned text models are suitable for assistant-like chat, while the pretrained models can adapt to various natural language generation tasks. The Llama 3.1 models also support using their outputs to improve other models, including synthetic data generation and distillation. Llama 3.1 is an autoregressive language model using an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "claude-2.0": "Claude 2 offers advancement in key capabilities for enterprises, including industry-leading 200K token context window, significantly reduced model hallucination rates, system prompts, and a new testing capability: tool calls.",
      "hunyuan-turbo": "A preview version of Hunyuan's new generation large language model, adopting a new Mixture of Experts (MoE) structure, offering faster inference efficiency and better performance compared to hunyuan-pro.",
      "gpt-4.5-preview": "The research preview version of GPT-4.5, which is our largest and most capable GPT model to date. It has broad world knowledge and can better understand user intent, making it excel at creative tasks and autonomous planning. GPT-4.5 accepts text and image inputs and generates text outputs (including structured outputs). It supports key developer features like function calling, batch API, and streaming output. GPT-4.5 particularly excels at tasks requiring creativity, open-ended thinking, and dialogue (like writing, learning, or exploring new ideas). Knowledge cutoff: October 2023.",
      "deepseek-ai/DeepSeek-V3": "DeepSeek-V3 is a mixture-of-experts (MoE) language model with 6.71 trillion parameters, using multi-head attention (MLA) and DeepSeekMoE architecture, combined with an auxiliary loss-free load balancing strategy, optimizing inference and training efficiency. Through pretraining on 148 trillion high-quality tokens, and incorporating supervised fine-tuning and reinforcement learning, DeepSeek-V3 surpasses other open-source models in performance, approaching leading closed-source models.",
      "anthropic/claude-3.5-sonnet": "Claude 3.5 Sonnet offers capabilities beyond Opus with faster speeds than Sonnet while maintaining the same price point. Sonnet particularly excels at programming, data science, visual processing, and agency tasks.",
      "QwQ-32B-Preview": "Qwen QwQ is an experimental research model developed by the Qwen team, focusing on enhancing AI reasoning capabilities.",
      "internlm3-latest": "Our latest model series, featuring superior reasoning performance, outperforming same-scale open-source models. By default points to our latest released InternLM3 series model, currently pointing to internlm3-8b-instruct.",
      "Doubao-1.5-thinking-vision-pro": "A new visual deep thinking model with enhanced general multimodal understanding and reasoning capabilities, achieving SOTA performance in 37 out of 59 public benchmarks.",
      "learnlm-2.0-flash-experimental": "LearnLM is an experimental, task-specific language model trained to follow learning science principles, capable of following systematic instructions in teaching and learning scenarios, serving as an expert tutor and more.",
      "aya": "Aya 23 is a multilingual model from Cohere that supports 23 languages, providing convenience for multilingual applications.",
      "accounts/fireworks/models/llama-v3p1-8b-instruct": "Meta Llama 3.1 series is a collection of language large language models (LLM) including 8B, 70B, and 405B parameter sizes of pretrained and instruction fine-tuned generative models. The Llama 3.1 instruction fine-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue applications and exceed many existing open and closed source chat models in common industry benchmarks.",
      "Doubao-1.5-thinking-pro-m": "Doubao-1.5 new deep thinking model (m version with native multimodal deep reasoning capabilities) shows outstanding performance in professional fields such as mathematics, programming, scientific reasoning, and general tasks like creative writing, reaching or approaching industry-leading level in multiple authoritative benchmarks including AIME 2024, Codeforces, and GPQA. Supports 128k context window and 16k output.",
      "mistralai/Mistral-7B-v0.1": "Mistral 7B is a compact yet high-performing model proficient in batch processing and simple tasks like classification and text generation, with good reasoning capabilities.",
      "qwen/qwen3-14b:free": "Qwen3-14B is a dense 148 billion parameter foundation language model in the Qwen3 series, designed for complex reasoning and efficient dialogue. It supports seamless switching between a 'thinking' mode for tasks like math, programming, and logical reasoning, and a 'non-thinking' mode for general conversation. Fine-tuned, it can be used for instruction following, tool agent usage, creative writing, and multilingual tasks across 100+ languages and dialects. It natively processes 32K token context and can be extended to 131K tokens using YaRN-based extension.",
      "gpt-4o-2024-05-13": "ChatGPT-4o is a dynamic model that updates in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
      "Pro/deepseek-ai/DeepSeek-V3-1226": "DeepSeek-V3 is a mixture-of-experts (MoE) language model with 6.71 trillion parameters, using multi-head attention (MLA) and DeepSeekMoE architecture, combined with an auxiliary loss-free load balancing strategy, optimizing inference and training efficiency. Through pretraining on 148 trillion high-quality tokens, and incorporating supervised fine-tuning and reinforcement learning, DeepSeek-V3 surpasses other open-source models in performance, approaching leading closed-source models.",
      "glm-4-long": "GLM-4-Long supports ultra-long text input, suitable for memory-based tasks and large-scale document processing.",
      "anthropic.claude-v2:1": "An updated version of Claude 2 with double the context window, and improvements in reliability, hallucination rates, and evidence-based accuracy across long documents and RAG contexts.",
      "llama-3.3-instruct": "Llama 3.3 instruction fine-tuned model is optimized for dialogue scenarios, surpassing many existing open-source chat models in common industry benchmarks.",
      "charglm-4": "CharGLM-4 is specifically designed for role-playing and emotional companionship, supporting ultra-long multi-turn memory and personalized dialogue, with wide applications.",
      "Qwen2.5-14B-Instruct": "The open-source 14B-scale model of Qianwen 2.5.",
      "gemini-1.5-flash-8b-exp-0924": "Gemini 1.5 Flash 8B 0924 is the latest experimental model with significant performance improvements in both text and multimodal use cases.",
      "c4ai-aya-expanse-8b": "Aya Expanse is a high-performance 8B multilingual model that aims to challenge monolingual model performance through innovations in instruction tuning, data curation, bias training, and model merging. It supports 23 languages.",
      "glm-4v": "GLM-4V provides powerful image understanding and reasoning capabilities, supporting various visual tasks.",
      "glm-4v-plus-0111": "GLM-4V-Plus features understanding capabilities for video content and multiple images, suitable for multimodal tasks.",
      "Phi-3-small-8k-instruct": "A 70 billion parameter model, superior to Phi-3-mini, focusing on high-quality, reasoning-intensive data.",
      "glm-z1-airx": "Ultra-fast inference: Features super-fast inference speed and powerful reasoning effects.",
      "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": "Llama 3 8B Instruct Turbo is a high-performance large language model supporting a wide range of application scenarios.",
      "deepseek-chat": "New open-source model integrating general and code capabilities - Not only retains the general dialogue capabilities of the original Chat model and the powerful code processing capabilities of the Coder model but also better aligns with human preferences. Additionally, DeepSeek-V2.5 has achieved significant improvements in writing tasks, instruction following, and other aspects.",
      "deepseek/deepseek-r1-distill-llama-8b": "DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, trained using outputs from DeepSeek R1.",
      "deepseek/deepseek-v3": "DeepSeek-V3 has achieved a breakthrough in inference speed compared to previous models. Ranking first among open-source models and comparable to the world's most advanced closed-source models. DeepSeek-V3 adopts multi-head attention (MLA) and DeepSeekMoE architecture, which have been fully validated in DeepSeek-V2. Additionally, DeepSeek-V3 pioneered an auxiliary loss-free strategy for load balancing and set multiple target pre-training objectives to achieve stronger performance.",
      "thudm/glm-z1-32b:free": "GLM-Z1-32B-0414 is an enhanced inference variant of GLM-4-32B, specifically constructed for deep mathematical, logical, and code-oriented problem solving. It applies extended reinforcement learning (task-specific and general competency-based preferences) to improve performance on complex multi-step tasks. Compared to the base GLM-4-32B model, Z1 significantly enhances capabilities in structured reasoning and formal domains.\n\nThe model supports forced execution of 'thinking' steps through prompt engineering and provides improved consistency for long-format outputs. It is optimized for agent workflows and supports long contexts (via YaRN), JSON tool calls, and fine-grained sampling configuration for stable inference. Particularly suitable for use cases requiring deep deliberation, step-by-step reasoning, or formal inference.",
      "codegeex4-all-9b": "CodeGeeX4-ALL-9B is a multilingual code generation model supporting comprehensive features including code completion and generation, code interpreter, web search, function calling, and repository-level code Q&A, covering various software development scenarios. It is a top-tier code generation model with parameters under 10B.",
      "google/gemma-3-27b-it": "Gemma 3 27B is Google's open-source language model that sets new standards in both efficiency and performance.",
      "hunyuan-functioncall": "Hunyuan's latest MOE architecture FunctionCall model, trained on high-quality FunctionCall data, with a 32K context window, leading in multiple evaluation metrics.",
      "deepseek-r1-fast-online": "DeepSeek R1 fast version, supporting real-time internet search, combining the powerful capabilities of 6.71B parameters with faster response speed.",
      "internvl3-latest": "Our latest released multimodal large model, featuring stronger image-text understanding capabilities, long sequence image understanding capabilities, with performance comparable to top closed-source models. By default points to our latest released InternVL series model, currently pointing to internvl3-78b.",
      "qwen3-14b": "Qwen3 is a new generation of Qianwen large model with significantly enhanced capabilities, achieving industry-leading performance in core abilities such as reasoning, general usage, Agent, and multilingual support, and supports switching between thinking modes.",
      "o3": "o3 is a fully capable powerful model that excels in multiple domains. It sets new standards for mathematics, science, programming, and visual reasoning tasks. It also excels at technical writing and instruction following. Users can utilize it to analyze text, code, and images, solving complex problems step by step.",
      "hunyuan-turbos-longtext-128k-20250325": "Proficient in handling long-form tasks such as document summarization and document Q&A, while also capable of processing general text generation tasks. Shows excellent performance in long text analysis and generation, effectively handling complex and comprehensive long content processing requirements.",
      "nvidia/llama-3.1-nemotron-70b-instruct": "Llama-3.1-Nemotron-70B-Instruct is NVIDIA's customized large language model aimed at improving the helpfulness of LLM responses.",
      "anthropic.claude-v2": "A model by Anthropic that demonstrates high capability across a wide range of tasks from complex dialogue and creative content generation to detailed instruction following.",
      "SenseChat-5-beta": "Partially performs better than SenseCat-5-1202",
      "deepseek-r1-distill-qianfan-llama-8b": "First released on April 14, 2025, distilled by the Qianfan large model research team using Llama3_8B as the base model (Built with Meta Llama), with Qianfan's corpus also added to the distillation data.",
      "command-r-plus-04-2024": "command-r-plus is an alias for command-r-plus-04-2024, so if you use command-r-plus in the API, it actually points to this model.",
      "openai/o3-mini": "o3-mini provides high intelligence within the same cost and latency targets as o1-mini.",
      "deepseek/deepseek-chat": "New open-source model integrating general and code capabilities - Not only retains the general dialogue capabilities of the original Chat model and the powerful code processing capabilities of the Coder model but also better aligns with human preferences. Additionally, DeepSeek-V2.5 has achieved significant improvements in writing tasks, instruction following, and other aspects.",
      "llama-3.2-11b-vision-instruct": "Excellent image reasoning capabilities on high-resolution images, suitable for visual understanding applications.",
      "generalv3": "Spark Pro is a high-performance large language model optimized for professional domains, focusing on mathematics, programming, medicine, education, and other fields, and supports internet search and built-in plugins for weather, dates, etc. Its optimized model shows excellent performance and high efficiency in complex knowledge Q&A, language understanding, and high-level text creation, making it an ideal choice for professional application scenarios.",
      "chatglm3": "ChatGLM3 is an open-source model released by Zhipu AI and Tsinghua KEG Lab, trained on massive Chinese-English labeled data and aligned with human preferences. Compared to the first-generation model, it achieved improvements of 16%, 36%, and 180% on MMLU, C-Eval, and GSM8K respectively, and topped the Chinese task benchmark C-Eval. Suitable for scenarios requiring high knowledge volume, reasoning ability, and creativity, such as advertising copy, novel writing, knowledge-based writing, code generation, etc.",
      "hunyuan-t1-20250321": "Comprehensively builds model cultural and scientific capabilities, with strong long-text information capture ability. Supports reasoning and solving scientific problems of various difficulties in mathematics/logic reasoning/science/code, etc.",
      "meta/llama-3.1-70b-instruct": "Empowered with complex dialogue capabilities, featuring excellent context understanding, reasoning ability, and text generation capabilities.",
      "hunyuan-standard": "Adopts better routing strategies while alleviating load balancing and expert consensus issues. In terms of long text, the massive recall indicator reaches 99.9%. MOE-32K offers relatively higher cost-effectiveness, achieving a balance between effects and price while enabling processing of ultra-long text inputs.",
      "meta-llama-3-8b-instruct": "A versatile 70 billion parameter model optimized for dialogue and text generation tasks.",
      "qwen2": "Qwen2 is Alibaba's new generation of large-scale language models, supporting diverse application needs with excellent performance.",
      "Baichuan4": "Model capabilities rank first domestically, surpassing mainstream foreign models in Chinese tasks such as knowledge encyclopedia, long text, and creative generation. Also possesses industry-leading multimodal capabilities, showing excellent performance across multiple authoritative evaluation benchmarks.",
      "Phi-3-medium-4k-instruct": "A 140 billion parameter model, superior to Phi-3-mini, focusing on high-quality, reasoning-intensive data.",
      "deepseek/deepseek-chat-v3-0324": "DeepSeek V3 is a 6.85B parameter expert mixture model, the latest iteration in the DeepSeek team's flagship chat model series.\n\nIt builds upon the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and excels across various tasks.",
      "Doubao-1.5-vision-pro": "Doubao-1.5-vision-pro is a newly upgraded multimodal large model supporting arbitrary resolution and extreme aspect ratio image recognition, strengthening visual reasoning, document recognition, detail information understanding, and instruction following capabilities.",
      "Baichuan4-Turbo": "Model capabilities rank first domestically, surpassing mainstream foreign models in Chinese tasks such as knowledge encyclopedia, long text, and creative generation. Also possesses industry-leading multimodal capabilities, showing excellent performance across multiple authoritative evaluation benchmarks.",
      "Skylark2-lite-8k": "Second generation Skylark model, Skylark2-lite model has higher response speed, suitable for scenarios requiring real-time response, cost sensitivity, and moderate model accuracy requirements, with an 8k context window length.",
      "text-embedding-3-large": "The most powerful vectorization model, suitable for both English and non-English tasks.",
      "qwen2.5-math-1.5b-instruct": "The Qwen-Math model has powerful mathematical problem-solving capabilities.",
      "llava:13b": "LLaVA is a multimodal model combining visual encoders and Vicuna for powerful visual and language understanding.",
      "hunyuan-standard-vision": "Hunyuan's latest multimodal model, supporting multiple types of tasks, with balanced Chinese and English capabilities.",
      "glm-4-plus": "GLM-4-Plus, as a high-intelligence flagship, possesses powerful capabilities in processing long text and complex tasks, with comprehensive performance improvements.",
      "thudm/glm-z1-32b": "GLM-Z1-32B-0414 is an enhanced inference variant of GLM-4-32B, specifically constructed for deep mathematical, logical, and code-oriented problem solving. It applies extended reinforcement learning (task-specific and general competency-based preferences) to improve performance on complex multi-step tasks. Compared to the base GLM-4-32B model, Z1 significantly enhances capabilities in structured reasoning and formal domains.\n\nThe model supports forced execution of 'thinking' steps through prompt engineering and provides improved consistency for long-format outputs. It is optimized for agent workflows and supports long contexts (via YaRN), JSON tool calls, and fine-grained sampling configuration for stable inference. Particularly suitable for use cases requiring deep deliberation, step-by-step reasoning, or formal inference.",
      "gpt-4o-realtime-preview-2024-12-17": "GPT-4o real-time version, supporting audio and text real-time input and output",
      "llama-3.2-90b-vision-preview": "Llama 3.2 is designed to handle tasks combining visual and textual data. It excels in image description and visual question answering tasks, bridging the gap between language generation and visual reasoning.",
      "codegeex-4": "CodeGeeX-4 is a powerful AI programming assistant, supporting intelligent Q&A and code completion in multiple programming languages, improving development efficiency.",
      "abab5.5-chat": "Focused on productivity scenarios, supporting complex task processing and efficient text generation, suitable for professional domain applications.",
      "us.anthropic.claude-3-5-sonnet-20241022-v2:0": "Claude 3.5 Sonnet raises industry standards, outperforming competitive peer models and Claude 3 Opus in extensive evaluations, while maintaining our mid-tier models' speed and cost profile.",
      "claude-3-5-sonnet-20241022": "Claude 3.5 Sonnet offers capabilities beyond Opus with faster speeds than Sonnet while maintaining the same price point. Sonnet particularly excels at programming, data science, visual processing, and agency tasks.",
      "meta-llama-3-70b-instruct": "A powerful 700 billion parameter model that excels in reasoning, coding, and broad language applications.",
      "anthropic/claude-3-opus": "Claude 3 Opus is Anthropic's most capable model for handling highly complex tasks. It demonstrates superior performance in capabilities, intelligence, fluency, and comprehension.",
      "meta.llama3-1-8b-instruct-v1:0": "Updated version of Meta Llama 3.1 8B Instruct featuring expanded 128K context length, multilinguality, and improved reasoning capabilities. The Llama 3.1 family of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in sizes of 8B, 70B, and 405B (text input/output). The Llama 3.1 instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and exceed many available open-source chat models in common industry benchmarks. Llama 3.1 is intended for commercial and research use in multiple languages. The instruction-tuned text models are suitable for assistant-like chat, while the pretrained models can adapt to various natural language generation tasks. The Llama 3.1 models also support using their outputs to improve other models, including synthetic data generation and distillation. Llama 3.1 is an autoregressive language model using an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "ernie-4.0-turbo-128k": "Baidu's self-developed flagship super large-scale language model, showing excellent comprehensive performance, widely applicable to complex task scenarios in various fields; supports automatic connection with Baidu search plugin, ensuring Q&A information timeliness. Performs better than ERNIE 4.0 in performance.",
      "meta/llama-3.2-1b-instruct": "Advanced cutting-edge small language model with language understanding, superior reasoning capabilities, and text generation abilities.",
      "us.anthropic.claude-3-7-sonnet-20250219-v1:0": "Claude 3.7 sonnet is Anthropic's fastest next-generation model. Compared to Claude 3 Haiku, Claude 3.7 Sonnet shows improvements across all capabilities and surpasses the previous generation's largest model Claude 3 Opus in many intelligence benchmark tests.",
      "mistralai/Mixtral-8x7B-v0.1": "Mixtral 8x7B is a sparse expert model that uses multiple parameters to improve inference speed, suitable for handling multilingual and code generation tasks.",
      "qwen2.5": "Qwen2.5 is Alibaba's new generation of large-scale language models, supporting diverse application needs with excellent performance.",
      "glm-z1-air": "Reasoning model: Possesses powerful reasoning capabilities, suitable for tasks requiring deep reasoning.",
      "mistral-large-latest": "Mistral Large is a flagship model excelling in multilingual tasks, complex reasoning, and code generation, making it an ideal choice for high-end applications.",
      "hunyuan-role": "Hunyuan's latest version role-playing model, officially fine-tuned and released by Hunyuan based on their model combined with role-playing scenario datasets for enhancement, showing better baseline effects in role-playing scenarios.",
      "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": "405B Llama 3.1 Turbo model providing ultra-large context support for big data processing, showing outstanding performance in ultra-large-scale artificial intelligence applications.",
      "meta-llama/llama-3.2-11b-vision-instruct": "LLaMA 3.2 is designed to handle tasks combining visual and textual data. It excels in image description and visual question answering tasks, bridging the gap between language generation and visual reasoning.",
      "glm-4-air-250414": "GLM-4-Air is a cost-effective version with performance close to GLM-4, offering fast speed at an affordable price.",
      "Doubao-pro-32k": "The most effective flagship model, suitable for handling complex tasks, showing great results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports reasoning and fine-tuning with a 32k context window.",
      "qwen2.5-omni-7b": "Qwen-Omni series models support input of multiple modalities including video, audio, images, and text, and can output both audio and text.",
      "meta/llama-3.3-70b-instruct": "Advanced LLM excelling in reasoning, mathematics, common sense, and function calling.",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": "DeepSeek-R1-Distill-Qwen-32B is a model obtained through knowledge distillation based on Qwen2.5-32B. The model was fine-tuned using 800,000 high-quality samples generated by DeepSeek-R1, showing outstanding performance across multiple domains including mathematics, programming, and reasoning. It achieved excellent results in multiple benchmark tests including AIME 2024, MATH-500, and GPQA Diamond, notably reaching 94.3% accuracy on MATH-500, demonstrating powerful mathematical reasoning capabilities.",
      "gemma2:2b": "Gemma 2 is an efficient model from Google covering various application scenarios from small-scale applications to complex data processing.",
      "gpt-4-32k": "GPT-4 provides a larger context window, capable of processing longer text inputs, suitable for scenarios requiring extensive information integration and data analysis.",
      "phi3": "Phi-3 is Microsoft's lightweight open model suitable for efficient integration and large-scale knowledge reasoning.",
      "google/gemma-2-2b-it": "Advanced small-scale language generation AI model for edge applications.",
      "deepseek-r1-distill-llama": "deepseek-r1-distill-llama is a model distilled from Llama using DeepSeek-R1.",
      "hunyuan-t1-latest": "The industry's first super large-scale Hybrid-Transformer-Mamba inference model, extending reasoning capabilities with ultra-fast decoding speed, further aligning with human preferences.",
      "deepseek/deepseek-r1-distill-qwen-14b": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, trained using outputs from DeepSeek R1. This model outperforms OpenAI's o1-mini in multiple benchmarks, achieving state-of-the-art results for dense models. Here are some benchmark results:\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nThe model, fine-tuned using outputs from DeepSeek R1, demonstrates competitive performance comparable to larger frontier models.",
      "codellama:34b": "Code Llama is an LLM focused on code generation and discussion, combining broad programming language support, suitable for developer environments.",
      "open-mistral-nemo": "Mistral Nemo is a 12B model developed in collaboration with Nvidia, providing excellent reasoning and coding capabilities, easy to integrate and replace.",
      "llama3-70b-8192": "Meta Llama 3 70B provides unparalleled complexity handling capabilities, custom-tailored for high-requirement projects.",
      "deepseek-ai/deepseek-llm-67b-chat": "DeepSeek LLM Chat (67B) is an innovative AI model providing deep language understanding and interaction capabilities.",
      "Pro/Qwen/Qwen2-1.5B-Instruct": "Qwen2-1.5B-Instruct is an instruction-tuned large language model in the Qwen2 series with 1.5B parameters. Based on the Transformer architecture, it uses SwiGLU activation function, attention QKV bias, and grouped query attention technologies. It shows excellent performance in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning across multiple benchmarks, surpassing most open-source models. Compared to Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct shows significant performance improvements in MMLU, HumanEval, GSM8K, C-Eval, and IFEval tests, despite having slightly fewer parameters",
      "hunyuan-large-longcontext": "Proficient in handling long-form tasks such as document summarization and document Q&A, while also capable of processing general text generation tasks. Shows excellent performance in long text analysis and generation, effectively handling complex and comprehensive long content processing requirements.",
      "Doubao-1.5-vision-pro-32k": "Doubao-1.5-vision-pro is a newly upgraded multimodal large model supporting arbitrary resolution and extreme aspect ratio image recognition, strengthening visual reasoning, document recognition, detail information understanding, and instruction following capabilities.",
      "mistral-small-latest": "Mistral Small is a cost-effective, fast, and reliable option suitable for translation, summarization, and sentiment analysis use cases.",
      "yi-large-fc": "Builds upon and strengthens the tool calling capabilities of the yi-large model, suitable for various business scenarios requiring agent or workflow construction.",
      "sonar-reasoning": "Advanced search product supporting search context, supporting advanced queries and follow-ups.",
      "Qwen2-VL-72B": "Qwen2-VL-72B is a powerful visual language model supporting multimodal processing of images and text, capable of accurately identifying image content and generating relevant descriptions or answers.",
      "Pro/Qwen/Qwen2-VL-7B-Instruct": "Qwen2-VL-7B-Instruct is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks including MathVista, DocVQA, RealWorldQA, and MTVQA. Qwen2-VL can be used for high-quality video-based Q&A, dialogue, and content creation, and also has complex reasoning and decision-making capabilities, capable of integrating with mobile devices, robots, etc., performing automatic operations based on visual environments and text instructions. In addition to English and Chinese, Qwen2-VL now supports text understanding in different languages in images, including most European languages, Japanese, Korean, Arabic, and Vietnamese",
      "qwen2.5:72b": "Qwen2.5 is Alibaba's new generation of large-scale language models, supporting diverse application needs with excellent performance.",
      "grok-vision-beta": "The latest image understanding model capable of processing various types of visual information, including documents, charts, screenshots, and photos.",
      "phi3:14b": "Phi-3 is Microsoft's lightweight open model suitable for efficient integration and large-scale knowledge reasoning.",
      "google/gemma-2-27b-it": "Gemma 2 27B is a general-purpose large language model with excellent performance and broad application scenarios.",
      "baichuan/baichuan2-13b-chat": "Baichuan-13B is an open-source commercially usable large language model with 13 billion parameters developed by Baichuan Intelligence, achieving the best results among models of similar size in both authoritative Chinese and English benchmarks",
      "doubao-1.5-lite-32k": "Doubao-1.5-lite new generation lightweight model, with extremely fast response speed, achieving world-class results in both effect and latency.",
      "abab6.5g-chat": "Specially designed for multilingual human-device conversations, supporting high-quality dialogue generation in English and many other languages.",
      "command-r-08-2024": "command-r-08-2024 is an updated version of the Command R model, released in August 2024.",
      "Doubao-pro-4k": "The most effective flagship model, suitable for handling complex tasks, showing great results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports reasoning and fine-tuning with a 4k context window.",
      "Phi-3-medium-128k-instruct": "The same Phi-3-medium model but with a larger context size, suitable for RAG or few-shot prompting.",
      "meta-llama-3.1-405b-instruct": "Llama 3.1 instruction-tuned text models are optimized for multilingual dialogue use cases and exceed many available open and closed source chat models in common industry benchmarks.",
      "deepseek/deepseek-r1-distill-llama-70b": "DeepSeek R1 Distill Llama 70B is a large language model based on Llama3.3 70B. The model leverages fine-tuning with DeepSeek R1 outputs to achieve competitive performance comparable to large frontier models.",
      "Pro/Qwen/Qwen2.5-VL-7B-Instruct": "Qwen2.5-VL is a new member of the Qwen series with powerful visual understanding capabilities. It can analyze text, charts, and layouts in images, understand long videos and capture events. It can perform reasoning, operate tools, support multi-format object positioning and generate structured output. It has optimized video understanding with dynamic resolution and frame rate training, and improved visual encoder efficiency.",
      "deepseek-coder-33B-instruct": "DeepSeek Coder 33B is a code language model trained on 20 trillion data points, of which 87% is code and 13% is Chinese and English text. The model incorporates 16K window size and fill-in tasks, providing project-level code completion and snippet filling functionality.",
      "generalv3.5": "Spark Max is the most comprehensive version, supporting internet search and numerous built-in plugins. Its fully optimized core capabilities along with system role definition and function calling features make it perform exceptionally well in various complex application scenarios.",
      "qwen/qwen2.5-7b-instruct": "Qwen2.5-7B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 7B model shows significant improvements in coding and mathematics domains. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows significant improvements in instruction following, understanding structured data, and generating structured output (especially JSON).",
      "accounts/fireworks/models/llama-v3-70b-instruct": "Meta developed and released the Meta Llama 3 series of large language models (LLM), which includes text models with 8B and 70B parameter scales for pretraining and instruction fine-tuning generation. Llama 3 instruction-tuned models are optimized for dialogue application scenarios and outperform many existing open-source chat models in common industry benchmark tests.",
      "hunyuan-turbo-20241223": "This version optimization: data instruction scaling, greatly improving model general generalization capabilities; significantly enhancing mathematics, code, and logical reasoning abilities; optimizing text comprehension and word understanding related capabilities; improving text creation content generation quality",
      "qwq-32b": "QwQ is the reasoning model of the Qwen series. Compared to traditional instruction fine-tuning models, QwQ has thinking and reasoning capabilities. In downstream tasks, especially complex problems, it can achieve significantly enhanced performance. QwQ-32B is a medium-sized reasoning model whose performance can rival the most advanced reasoning models (such as DeepSeek-R1, o1-mini).",
      "mistralai/Mistral-7B-Instruct-v0.3": "Mistral (7B) Instruct v0.3 provides efficient computational capabilities and natural language understanding, suitable for broad applications.",
      "qwen-vl-max-latest": "Qianwen's super large-scale visual language model. Compared to the enhanced version, it further improves visual reasoning capabilities and instruction following abilities, providing higher levels of visual perception and cognition.",
      "SenseChat": "Base version model (V4), 4K context length, strong general capabilities",
      "SenseChat-Turbo-1202": "The latest lightweight version model, achieving over 90% of full-scale model capabilities while significantly reducing inference costs.",
      "amazon.titan-text-express-v1": "Amazon's Titan Text Express has a context length of up to 8,000 tokens, highly suitable for a wide range of advanced general language tasks, such as open-ended text generation and conversational chat, as well as support in retrieval-augmented generation (RAG). Upon release, the model was optimized for English, with the preview version also supporting over 100 other languages.",
      "meta-llama/Meta-Llama-3-8B-Instruct-Lite": "Llama 3 8B Instruct Lite is suitable for resource-constrained environments, providing excellent balanced performance.",
      "hunyuan-turbos-latest": "hunyuan-TurboS flagship large model latest version, featuring enhanced thinking capabilities and improved user experience.",
      "taichu_llm": "Trained on massive high-quality data, with stronger text understanding, content creation, dialogue and question answering capabilities",
      "yi-large-rag": "Advanced service based on yi-large super model, combining search and generation technology to provide accurate answers, real-time web search information service.",
      "o1-preview": "o1 is OpenAI's new inference model, suitable for complex tasks requiring broad general knowledge. This model has a 128K context window and knowledge cutoff date of October 2023.",
      "aya:35b": "Aya 23 is a multilingual model launched by Cohere, supporting 23 languages, providing convenience for multilingual application scenarios.",
      "tngtech/deepseek-r1t-chimera:free": "DeepSeek-R1T-Chimera was created by combining DeepSeek-R1 and DeepSeek-V3 (0324), combining R1's inference capabilities with V3's improved instruction efficiency. It is based on the DeepSeek-MoE Transformer architecture and has been optimized for general text generation tasks.\n\nThe model combines the pre-trained weights of two source models to balance performance in inference, efficiency and instruction following tasks. It is released under the MIT license, intended for research and commercial use.",
      "deepseek/deepseek-chat-v3-0324:free": "DeepSeek V3 is a 685B parameter expert mixture model, the latest iteration in the DeepSeek team's flagship chat model series.\n\nIt builds upon the DeepSeek V3 model and excels at various tasks.",
      "command-r": "Command R is an LLM optimized for dialogue and long context tasks, particularly suitable for dynamic interaction and knowledge management.",
      "meta.llama3-8b-instruct-v1:0": "Meta Llama 3 is an open large language model (LLM) for developers, researchers and enterprises, aimed at helping them build, experiment and responsibly scale their AI ideas. As part of a global community innovation infrastructure, it is particularly suitable for devices with limited computing power and resources, edge devices, and faster training times.",
      "qwen-long": "Tongyi Qianwen ultra-large language model, supporting long text context, as well as dialogue functions based on long documents, multiple documents and other scenarios.",
      "mixtral:8x22b": "Mixtral is Mistral AI's expert model, with open weights, and provides support for code generation and language understanding.",
      "anthropic.claude-3-haiku-20240307-v1:0": "Claude 3 Haiku is Anthropic's fastest and most cost-efficient model, providing near-instant response times. It can quickly answer simple queries and requests. Customers will be able to build seamless AI experiences that simulate human interaction. Claude 3 Haiku can process images and return text output, with a 200K context window.",
      "accounts/fireworks/models/deepseek-r1": "DeepSeek-R1 is one of the most advanced large language models, optimized through reinforcement learning and cold-start data, with excellent performance in reasoning, mathematics and programming.",
      "Doubao-pro-128k": "The most effective mainstream model, suitable for complex tasks, with excellent performance in reference Q&A, summarization, creation, text classification, role-playing and other scenarios. Supports 128k context window inference and fine-tuning.",
      "qwen-vl-ocr-latest": "Tongyi Qianwen OCR is a text extraction specific model, focused on text extraction capabilities from documents, tables, test papers, handwritten text and other types of images. It can recognize multiple languages, currently supporting: Chinese, English, French, Japanese, Korean, German, Russian, Italian, Vietnamese, Arabic.",
      "step-2-mini": "An ultra-fast large model based on the new generation self-attention architecture MFA, achieving similar results to step1 at extremely low cost, while maintaining higher throughput and faster response latency. Can handle general tasks, with strengths in code capabilities.",
      "meta-llama/Llama-2-13b-chat-hf": "LLaMA-2 Chat (13B) provides excellent language processing capabilities and outstanding interactive experience.",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": "Llama 3.1 Nemotron 70B is a large language model customized by NVIDIA, aimed at improving the helpfulness of LLM responses to user queries. The model excels in Arena Hard, AlpacaEval 2 LC and GPT-4-Turbo MT-Bench benchmarks, ranking first in all three automatic alignment benchmarks as of Oct 1, 2024. The model uses RLHF (particularly REINFORCE), Llama-3.1-Nemotron-70B-Reward and HelpSteer2-Preference prompts trained on the Llama-3.1-70B-Instruct model.",
      "o3-mini": "o3-mini is our latest small inference model, providing high intelligence under the same cost and latency targets as o1-mini.",
      "upstage/SOLAR-10.7B-Instruct-v1.0": "Upstage SOLAR Instruct v1 (11B) is suitable for refined instruction tasks, providing excellent language processing capabilities.",
      "Phi-3-mini-128k-instruct": "Same Phi-3-mini model but with larger context size, suitable for RAG or few-shot prompting.",
      "gemma2:27b": "Gemma 2 is Google's high-efficiency model, covering various application scenarios from small applications to complex data processing.",
      "step-1-8k": "Small model suitable for lightweight tasks.",
      "Pro/deepseek-ai/DeepSeek-R1": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses repetition and hallucination issues in models. Before RL, DeepSeek-R1 introduced cold-start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematics, code, and reasoning tasks, and has improved overall performance through carefully designed training methods.",
      "accounts/fireworks/models/mixtral-8x22b-instruct": "Mixtral MoE 8x22B Instruct v0.1 is the instruction fine-tuned version of Mixtral MoE 8x22B v0.1, with chat completion API enabled.",
      "mistral-small": "Mistral Small can be used for any language-based task requiring high efficiency and low latency.",
      "command-nightly": "To shorten the time between major version releases, we launched the nightly version of the Command model. For the Command series, this version is called command-nightly. Note that command-nightly is the newest, most experimental and (potentially) unstable version. Nightly versions are updated regularly without prior notice, so they are not recommended for use in production environments.",
      "glm-4-alltools": "GLM-4-AllTools is a multi-functional intelligence model, optimized to support complex instruction planning and tool calling, such as web browsing, code interpretation and text generation, suitable for multi-task execution.",
      "qwen/qwen3-14b": "Qwen3 is a new generation Tongyi Qianwen large model with greatly enhanced capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual, and supports thinking mode switching.",
      "llama-3.3-70b-versatile": "Meta Llama 3.3 multilingual large language model (LLM) is a 70B (text in/text out) pre-trained and instruction-tuned generation model. Llama 3.3 instruction-tuned pure text model has been optimized for multilingual dialogue use cases and outperforms many available open source and closed chat models on common industry benchmarks.",
      "nvidia/llama-3.1-nemotron-51b-instruct": "A unique language model offering unparalleled accuracy and efficiency performance.",
      "Qwen/Qwen2.5-72B-Instruct-128K": "Qwen2.5-72B-Instruct is one of the latest large language models released by Alibaba Cloud. This 72B model shows significant improvements in coding and mathematics capabilities. It supports inputs up to 128K tokens and can generate text exceeding 8K tokens. The model also provides multilingual support, covering over 129 languages including Chinese, English, etc. The model shows significant improvements in instruction following, structured data understanding, and generating structured output (especially JSON).",
      "Baichuan3-Turbo-128k": "Features 128K ultra-long context window, optimized for enterprise high-end scenarios, greatly improved effectiveness, high cost-performance ratio. Compared to Baichuan2 model, content creation improved by 30%, knowledge Q&A improved by 37%, role-playing ability improved by 40%. Overall performance better than GPT3.5.",
      "deepseek_r1": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses repetition and hallucination issues in models. Before RL, DeepSeek-R1 introduced cold-start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematics, code, and reasoning tasks, and has improved overall performance through carefully designed training methods.",
      "meta-llama-3.1-8b-instruct": "Llama 3.1 instruction-tuned text model, optimized for multilingual dialogue use cases, performs exceptionally well on common industry benchmarks among many available open source and closed chat models.",
      "deepseek-r1-70b-online": "DeepSeek R1 70B standard version, supporting real-time internet search, suitable for dialogues and text processing tasks requiring the latest information.",
      "InternVL2-8B": "InternVL2-8B is a powerful vision-language model that supports multimodal processing of images and text, capable of accurately identifying image content and generating relevant descriptions or answers.",
      "doubao-1.5-pro-32k": "Doubao-1.5-pro new generation mainstream model, performance comprehensively upgraded, showing excellence in knowledge, code, reasoning, etc.",
      "mistral-nemo": "Mistral Nemo, launched by Mistral AI and NVIDIA collaboration, is a high-performance 12B model.",
      "SenseChat-Vision": "Latest version model (V5.5), supports image input, comprehensive model foundation capability optimization, achieved significant improvements in object attribute recognition, spatial relationships, action event recognition, scene understanding, emotion recognition, logical common sense reasoning and text understanding generation.",
      "Meta-Llama-3.2-3B-Instruct": "Advanced cutting-edge small language model with language understanding, superior reasoning capabilities and text generation abilities.",
      "qwen2.5-vl-7b-instruct": "Instruction following, mathematics, problem-solving, and code overall improved, enhanced universal object recognition capability, supports accurate positioning of visual elements in multiple formats, supports understanding of long video files (up to 10 minutes) and event timestamp positioning at second-level, can understand temporal order and speed, supports OS or Mobile Agent based on analysis and positioning capabilities, strong key information extraction capability and Json format output capability, this version is 72B version, the most capable version in this series.",
      "hunyuan-lite": "Upgraded to MOE structure, with 256k context window, leading numerous open-source models in NLP, code, mathematics, industry and other evaluation sets.",
      "pro-128k": "Spark Pro 128K is configured with extra-large context processing capability, able to process up to 128K context information, particularly suitable for long content requiring comprehensive analysis and long-term logical association processing, can provide fluent and consistent logic and diverse reference support in complex text communication.",
      "mistral": "Mistral is Mistral AI's 7B model, suitable for diverse language processing needs.",
      "Qwen2-7B-Instruct": "Qwen2 is the new generation of large language model series launched by the Qwen team. It is based on the Transformer architecture and employs SwiGLU activation function, attention QKV bias, group query attention, mixture of sliding window attention and full attention. Additionally, the Qwen team has improved the tokenizer to adapt to multiple natural languages and code.",
      "google/gemma-2-9b": "Gemma 2 is Google's high-efficiency model, covering various application scenarios from small applications to complex data processing.",
      "gpt-4.1": "GPT-4.1 is our flagship model for complex tasks. It is particularly suitable for cross-domain problem solving.",
      "anthropic.claude-instant-v1": "A fast, economical yet still very capable model that can handle a range of tasks including everyday conversation, text analysis, summarization and document Q&A.",
      "gpt-4o-2024-11-20": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education and technical support.",
      "jina-deepsearch-v1": "Deep search combines web search, reading and reasoning to conduct comprehensive investigations. You can view it as an agent that accepts your research task - it will conduct broad searches and iterate multiple times before providing an answer. This process involves continuous research, reasoning and solving problems from various angles. This is fundamentally different from standard large models that generate answers directly from pre-training data and traditional RAG systems that rely on one-time surface searches.",
      "glm-4-air": "GLM-4-Air is a cost-effective version, with performance close to GLM-4, providing fast speed and affordable pricing.",
      "abab6.5t-chat": "Optimized for Chinese human dialogue scenarios, providing fluent and natural Chinese expression dialogue generation capabilities.",
      "meta-llama/llama-3.3-70b-instruct": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, delivering the performance of a 405B model at extremely low cost. Based on the Transformer architecture and enhanced through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) for improved usefulness and safety. Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open source and closed chat models on multiple industry benchmarks. Knowledge cutoff date is December 2023.",
      "ernie-char-fiction-8k": "Baidu's self-developed vertical scenario large language model, suitable for game NPCs, customer service dialogue, dialogue role-playing and other application scenarios, with clearer and more consistent persona design, stronger instruction following ability, and better reasoning performance.",
      "Doubao-lite-4k": "Features extremely fast response times, better cost-performance ratio, providing more flexible options for different customer scenarios. Supports reasoning and fine-tuning with 4k context window.",
      "claude-2.1": "Claude 2 provides enterprise advancements in key capabilities, including industry-leading 200K token context, significantly reduced model hallucination rates, system prompts, and a new test feature: tool calling.",
      "accounts/fireworks/models/phi-3-vision-128k-instruct": "Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model, built on datasets including synthetic data and curated public web datasets, focusing on high-quality, reasoning-dense data in text and visual aspects. The model belongs to the Phi-3 model family, with its multimodal version supporting 128K context length (in tokens). The model has undergone rigorous strengthening processes, including supervised fine-tuning and direct preference optimization, to ensure accurate instruction following and robust safety measures.",
      "grok-3-mini-fast-beta": "Lightweight model that thinks before responding. Runs fast and intelligently, suitable for logical tasks that don't require deep domain knowledge, and can access original thinking patterns.",
      "openai/o3": "o3 is a fully capable powerful model that excels in multiple domains. It sets new standards for mathematics, science, programming and visual reasoning tasks. It also excels at technical writing and instruction following. Users can leverage it to analyze text, code and images, solving complex problems step by step.",
      "accounts/fireworks/models/deepseek-v3": "Powerful Mixture-of-Experts (MoE) language model provided by Deepseek, with total parameters of 671B, activating 37B parameters per token.",
      "meta/llama-3.1-8b-instruct": "Advanced cutting-edge model with language understanding, superior reasoning capabilities and text generation abilities.",
      "google/gemma-2-9b-it:free": "Gemma 2 is Google's lightweight open-source text model series.",
      "yi-vision": "Complex vision task model providing high-performance image understanding and analysis capabilities.",
      "doubao-1.5-pro-256k": "Doubao-1.5-pro-256k is a comprehensively upgraded version based on Doubao-1.5-Pro, with overall performance improved by 10%. Supports inference with 256k context window, output length supports up to 12k tokens. Higher performance, larger window, ultra-high cost-performance ratio, suitable for broader application scenarios.",
      "max-32k": "Spark Max 32K is configured with large context processing capability, stronger context understanding and logical reasoning ability, supports 32K tokens text input, suitable for long document reading, proprietary knowledge Q&A and other scenarios",
      "qwen2.5-math-72b-instruct": "Qwen-Math model has powerful mathematical problem-solving capabilities.",
      "claude-3-7-sonnet-20250219": "Claude 3.7 Sonnet is Anthropic's most intelligent model to date, and also the first hybrid reasoning model in the market. Claude 3.7 Sonnet can generate near-instant responses or extended step-by-step thinking, with users able to clearly see these processes. Sonnet particularly excels at programming, data science, visual processing, and agent tasks.",
      "moonshot-v1-32k-vision-preview": "Kimi vision models (including moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview etc.) can understand image content, including image text, image colors and object shapes.",
      "qvq-max-latest": "Tongyi Qianwen QVQ visual reasoning model, supporting visual input and thought chain output, demonstrating stronger capabilities in mathematics, programming, visual analysis, creation and general tasks.",
      "step-2-16k-exp": "Experimental version of the step-2 model, including latest features, rolling updates. Not recommended for use in formal production environments.",
      "r1-1776": "R1-1776 is a version of the DeepSeek R1 model, post-trained to provide unverified, unbiased factual information.",
      "megrez-3b-instruct": "Megrez-3B-Instruct is a large language model fully independently trained by WuDao. Megrez-3B-Instruct aims to create an ultra-fast reasoning, small but sophisticated, extremely easy-to-use edge intelligence solution through the concept of hardware-software coordination.",
      "meta-llama/llama-3-8b-instruct": "Llama 3 8B Instruct optimized for high-quality dialogue scenarios, performing better than many open-source models.",
      "step-1-32k": "Supports medium-length dialogues, suitable for various application scenarios.",
      "gemini-2.0-flash-exp": "Gemini 2.0 Flash model variant, optimized for cost-effectiveness and low latency targets.",
      "yi-large": "Brand new trillion parameter model, providing super strong Q&A and text generation capabilities.",
      "gemini-2.0-flash": "Gemini 2.0 Flash offers next-generation capabilities and improvements, including superior speed, native tool use, multimodal generation and 1M token context window.",
      "grok-3-fast-beta": "Flagship model excelling at data extraction, programming and text summarization for enterprise applications, possessing deep knowledge in finance, medical, legal and scientific domains.",
      "qwen-max-latest": "Tongyi Qianwen trillion-level ultra-large scale language model, supporting inputs in different languages including Chinese and English, currently the API model behind Tongyi Qianwen 2.5 product version.",
      "accounts/fireworks/models/llama-v3p1-405b-instruct": "Meta Llama 3.1 series is a collection of language large language models (LLM) including pre-trained and instruction fine-tuned generation models at 8B, 70B and 405B parameter scales. Llama 3.1 instruction fine-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue scenarios and outperform many existing open source and closed chat models on common industry benchmark tests. 405B is the most capable model in the Llama 3.1 family. The model uses FP8 for inference, closely matching reference implementation.",
      "gryphe/mythomax-l2-13b": "MythoMax-L2 (13B) is an innovative model suitable for multi-domain applications and complex tasks.",
      "gpt-3.5-turbo": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
      "abab5.5s-chat": "Designed specifically for Chinese human dialogue scenarios, providing high-quality Chinese dialogue generation capabilities, suitable for various application scenarios.",
      "meta-llama/Llama-3.2-3B-Instruct-Turbo": "LLaMA 3.2 aims to handle tasks combining visual and textual data. It excels in image description and visual question answering tasks, bridging the gap between language generation and visual reasoning.",
      "Phi-3.5-vision-instrust": "Updated version of the Phi-3-vision model.",
      "deepseek-r1-distill-qwen": "deepseek-r1-distill-qwen is a model distilled from Qwen to DeepSeek-R1.",
      "360gpt-pro": "360GPT Pro, as an important member of the 360 AI model series, satisfies diverse natural language application scenarios with efficient text processing capabilities, supporting long text understanding and multi-turn conversations.",
      "qwen2.5-coder-1.5b-instruct": "Open-source version of the Tongyi Qianwen code model.",
      "qwen2.5-14b-instruct-1m": "Tongyi Qianwen 2.5's open-source 72B scale model.",
      "qwen2:72b": "Qwen2 is Alibaba's new generation large-scale language model, supporting diverse application needs with superior performance.",
      "solar-mini": "Solar Mini is a compact LLM that outperforms GPT-3.5, featuring powerful multilingual capabilities supporting English and Korean, providing efficient and elegant solutions.",
      "o1": "o1 is OpenAI's new inference model, supporting image and text input with text output, suitable for complex tasks requiring broad general knowledge. This model has 200K context and knowledge cutoff date of October 2023.",
      "ERNIE-4.0-8K-Preview": "Baidu's self-researched flagship super-large language model, achieving comprehensive model capability upgrade compared to ERNIE 3.5, widely applicable to complex task scenarios across various domains; supports automatic connection to Baidu search plugin, ensuring timely Q&A information.",
      "gemini-2.5-pro-preview-05-06": "Gemini 2.5 Pro Preview is Google's most advanced thinking model, capable of reasoning about complex problems in code, mathematics and STEM fields, as well as using long context to analyze large datasets, code repositories and documents.",
      "qwen-vl-chat-v1": "Tongyi Qianwen VL supports flexible interaction methods, including multi-image, multi-turn Q&A, creation and other capabilities.",
      "gpt-4-1106-preview": "The latest GPT-4 Turbo model has visual capabilities. Now, visual requests can use JSON mode and function calling. GPT-4 Turbo is a powerful version providing cost-effective support for multimodal tasks. It finds a balance between accuracy and efficiency, suitable for application scenarios requiring real-time interaction.",
      "gpt-35-turbo-16k": "GPT 3.5 Turbo 16k, high-capacity text generation model suitable for complex tasks.",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": "DeepSeek-R1 distillation model, optimizing reasoning performance through reinforcement learning and cold-start data, open-source model refreshing task benchmarks.",
      "tts-1": "Latest text-to-speech model, optimized for real-time scenarios",
      "qwen2:1.5b": "Qwen2 is Alibaba's new generation large-scale language model, supporting diverse application needs with superior performance.",
      "accounts/fireworks/models/qwen2-vl-72b-instruct": "The 72B version of the Qwen-VL model is Alibaba's latest iteration, representing innovations from the past year.",
      "Qwen/Qwen2.5-72B-Instruct-Turbo": "Qwen2.5 is a brand new large language model series aimed at optimizing instruction-based task processing.",
      "step-1-256k": "Has ultra-long context processing capability, particularly suitable for long document analysis.",
      "wizardlm2:8x22b": "WizardLM 2 is a language model provided by Microsoft AI, excelling particularly in complex dialogue, multilingual, reasoning, and intelligent assistant domains.",
      "c4ai-aya-vision-32b": "Aya Vision is a state-of-the-art multimodal model that excels in multiple key benchmarks in language, text, and image capabilities. It supports 23 languages. This 320 billion parameter version focuses on advanced multilingual performance.",
      "Doubao-vision-pro-32k": "Doubao-vision model is a multimodal large model launched by Doubao, featuring powerful image understanding and reasoning capabilities, as well as precise instruction understanding ability. The model has demonstrated strong performance in image text information extraction and image-based reasoning tasks, capable of handling more complex and broader visual question answering tasks.",
      "open-codestral-mamba": "Codestral Mamba is a Mamba 2 language model focused on code generation, providing powerful support for advanced code and reasoning tasks.",
      "learnlm-1.5-pro-experimental": "LearnLM is an experimental, task-specific language model, trained to conform to learning science principles, capable of following system instructions in teaching and learning scenarios, serving as an expert tutor, etc.",
      "qwq": "QwQ is the reasoning model of the Qwen series. Compared to traditional instruction-tuned models, QwQ has the ability to think and reason, capable of significantly improving performance in downstream tasks, especially on difficult problems. QwQ-32B is a medium-sized reasoning model that can achieve comparable performance when competing with the most advanced reasoning models (such as DeepSeek-R1, o1-mini).",
      "gemma-7b-it": "Gemma 7B is suitable for small-scale task processing, combining cost-effectiveness.",
      "Baichuan2-Turbo": "Uses search enhancement technology to achieve comprehensive linking of large models with domain knowledge and global knowledge. Supports uploading various documents like PDF, Word, etc., and URL input, with timely and comprehensive information retrieval, accurate and professional output results.",
      "yi-vision-v2": "Complex vision task model, providing high-performance understanding and analysis capabilities based on multiple images.",
      "meta-llama/llama-3.3-70b-instruct:free": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, delivering the performance of a 405B model at extremely low cost. Based on the Transformer architecture and enhanced through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) for improved usefulness and safety. Its instruction-tuned version is optimized for multilingual dialogue and outperforms many open source and closed chat models on multiple industry benchmarks. Knowledge cutoff date is December 2023.",
      "glm-4": "GLM-4 is the flagship version released in January 2024, currently replaced by the stronger GLM-4-0520.",
      "claude-3-5-sonnet-20240620": "Claude 3.5 Sonnet provides capabilities exceeding Opus and faster speed than Sonnet while maintaining the same price as Sonnet. Sonnet particularly excels at programming, data science, visual processing, and agent tasks.",
      "mistralai/Mistral-7B-Instruct-v0.1": "Mistral (7B) Instruct is known for high performance, suitable for various language tasks.",
      "ai21-jamba-1.5-mini": "A multilingual model with 1.2B parameters (1.2B active), providing 56K long context window, function calling, structured output and fact-based generation.",
      "kimi-latest": "Kimi intelligent assistant product uses the latest Kimi large model, which may contain unstable features. Supports image understanding, and will automatically choose 8k/32k/128k models as billing models based on the context length of the request",
      "compound-beta": "Compound-beta is a hybrid AI system supported by multiple open available models already supported in GroqCloud, capable of intelligently and selectively using tools to answer user queries.",
      "claude-3-haiku-20240307": "Claude 3 Haiku is Anthropic's fastest and most cost-efficient model, aimed at achieving near-instant responses. It has fast and accurate directional capabilities.",
      "codellama:70b": "Code Llama is an LLM specialized in code generation and discussion, combining broad programming language support, suitable for developer environments.",
      "google/gemini-flash-1.5": "Gemini 1.5 Flash provides optimized multimodal processing capabilities, suitable for various complex task scenarios.",
      "c4ai-aya-vision-8b": "Aya Vision is a state-of-the-art multimodal model that excels in multiple key benchmarks in language, text, and image capabilities. This 80 billion parameter version focuses on low latency and optimal performance.",
      "qwen3-32b": "Qwen3 is a new generation Tongyi Qianwen large model with greatly enhanced capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual, and supports thinking mode switching.",
      "openai/gpt-4.1-mini": "GPT-4.1 mini provides a balance between intelligence, speed and cost, making it an attractive model for many use cases.",
      "hunyuan-vision": "Latest multimodal model from Hunyuan, supporting image+text input to generate text content.",
      "lite": "Spark Lite is a lightweight large language model featuring ultra-low latency and efficient processing capabilities, completely free and open, supporting real-time online search functionality. Its fast response characteristic makes it perform excellently in inference applications and model fine-tuning on low-compute devices, bringing excellent cost-effectiveness and intelligent experience to users, particularly showing strong performance in knowledge Q&A, content generation and search scenarios.",
      "o4-mini": "o4-mini is our latest small o-series model. It is optimized for fast and effective inference, showing extremely high efficiency and performance in coding and visual tasks.",
      "Qwen2.5-Coder-32B-Instruct": "Advanced LLM supporting code generation, reasoning and fixes, covering mainstream programming languages.",
      "Pro/deepseek-ai/DeepSeek-V3": "DeepSeek-V3 is a Mixture-of-Experts (MoE) language model with 671B parameters, using multi-head linear attention (MLA) and DeepSeekMoE architecture, combined with load balancing strategy without auxiliary loss, optimizing inference and training efficiency. Through pre-training on 148T high-quality tokens, and undergoing supervised fine-tuning and reinforcement learning, DeepSeek-V3 surpasses other open-source models in performance, approaching leading closed-source models.",
      "meta/llama-3.2-11b-vision-instruct": "Cutting-edge vision-language model, excelling at high-quality reasoning from images.",
      "gpt-4o-audio-preview": "GPT-4o Audio model, supporting audio input and output",
      "gpt-4o-mini": "GPT-4o mini is OpenAI's latest model after GPT-4 Omni, supporting image-text input and outputting text. As their most advanced small model, it is much more affordable than other recent frontier models, and over 60% more cost-effective than GPT-3.5 Turbo. It maintains cutting-edge intelligence while offering significant cost-effectiveness. GPT-4o mini scored 82% in MMLU tests, currently ranking higher than GPT-4 in chat preferences.",
      "qwen3-4b": "Qwen3 is a new generation Tongyi Qianwen large model with greatly enhanced capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual, and supports thinking mode switching.",
      "anthropic/claude-3.7-sonnet": "Claude 3.7 Sonnet is Anthropic's most intelligent model to date, and also the first hybrid reasoning model in the market. Claude 3.7 Sonnet can generate near-instant responses or extended step-by-step thinking, with users able to clearly see these processes. Sonnet particularly excels at programming, data science, visual processing, and agent tasks.",
      "wizardlm2": "WizardLM 2 is a language model provided by Microsoft AI, excelling particularly in complex dialogue, multilingual, reasoning, and intelligent assistant domains.",
      "meta-llama/llama-3.2-3b-instruct": "meta-llama/llama-3.2-3b-instruct",
      "llama3-groq-8b-8192-tool-use-preview": "Llama 3 Groq 8B Tool Use is a model optimized for efficient tool usage, supporting fast parallel computation.",
      "360gpt-pro-trans": "Translation-specific model, deeply fine-tuned and optimized, leading translation results.",
      "code-raccoon-v1": "Code Raccoon is a software intelligence research and development assistant based on large language models, covering software requirements analysis, architecture design, code writing, software testing and other phases, meeting various needs such as user code writing and programming learning. Code Raccoon supports 90+ mainstream programming languages including Python, Java, JavaScript, C++, Go, SQL and mainstream IDEs like VS Code, IntelliJ IDEA. In practical applications, Code Raccoon can help developers improve programming efficiency by over 50%.",
      "gpt-4-32k-0613": "GPT-4 provides a larger context window, capable of processing longer text input, suitable for scenarios requiring extensive information integration and data analysis.",
      "llama3-groq-70b-8192-tool-use-preview": "Llama 3 Groq 70B Tool Use provides powerful tool calling capabilities, supporting efficient processing of complex tasks.",
      "qwen3-1.7b": "Qwen3 is a new generation Tongyi Qianwen large model with greatly enhanced capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual, and supports thinking mode switching.",
      "open-mixtral-8x7b": "Mixtral 8x7B is a sparse expert model, utilizing multiple parameters to improve inference speed, suitable for handling multilingual and code generation tasks.",
      "Baichuan4-Air": "Model capability ranks first domestically, surpassing mainstream foreign models in Chinese tasks such as encyclopedic knowledge, long text, and creative generation. It also possesses industry-leading multimodal capabilities, showing excellent performance in multiple authoritative evaluation benchmarks.",
      "ERNIE-3.5-128K": "Baidu's self-developed flagship large-scale language model, covering massive Chinese and English corpus, possessing powerful general capabilities, can meet the demands of most dialogue Q&A, creative generation, and plugin application scenarios; supports automatic connection to Baidu search plugin, ensuring Q&A information timeliness.",
      "meta-llama/llama-3.1-8b-instruct:free": "LLaMA 3.1 provides multilingual support and is one of the industry-leading generation models.",
      "THUDM/GLM-4-32B-0414": "GLM-4-32B-0414 is the new generation open-source model in the GLM series, with 320 billion parameters. This model's performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series.",
      "qwen-coder-plus-latest": "Tongyi Qianwen code model.",
      "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": "LLaMA 3.2 aims to handle tasks combining visual and textual data. It excels in image description and visual question answering tasks, bridging the gap between language generation and visual reasoning.",
      "text-embedding-3-small": "New generation efficient and economical Embedding model, suitable for knowledge retrieval, RAG applications and other scenarios",
      "meta/llama-3.2-3b-instruct": "Advanced cutting-edge small language model with language understanding, superior reasoning capabilities and text generation abilities.",
      "qwen/qwen3-8b:free": "Qwen3-8B is an 82B parameter causal language model in the Qwen3 series, designed specifically for reasoning-dense tasks and efficient dialogue. It supports seamless switching between a 'thinking' mode for mathematics, coding, and logical reasoning, and a 'non-thinking' mode for general conversation. The model is fine-tuned for instruction following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports 32K token context window and can be extended to 131K tokens using YaRN.",
      "jamba-large": "Our strongest and most advanced model, designed specifically for handling enterprise-level complex tasks, featuring superior performance.",
      "deepseek-v3": "DeepSeek-V3 is a powerful expert mixture (MoE) language model with a total of 671B parameters, activating 370B parameters per token.",
      "gpt-4o-mini-realtime-preview": "GPT-4o-mini real-time version, supporting audio and text real-time input and output",
      "google/gemma-2-27b": "Gemma 2 is Google's high-efficiency model, covering various application scenarios from small applications to complex data processing.",
      "yi-spark": "Small yet sophisticated, ultra-fast model. Provides enhanced mathematical computation and code writing capabilities.",
      "hunyuan-translation": "Supports bidirectional translation between Chinese and English, Japanese, French, Portuguese, Spanish, Turkish, Russian, Arabic, Korean, Italian, German, Vietnamese, Malay, Indonesian in 15 languages. Based on multi-scenario translation evaluation set automated evaluation COMET score, overall translation capability in over ten common language pairs surpasses market peer models.",
      "Vendor-A/Qwen/Qwen2.5-72B-Instruct": "Qwen2.5-72B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 72B model shows significant improvements in coding and mathematics domains. The model also provides multilingual support, covering over 129 languages including Chinese, English, etc. The model shows significant improvements in instruction following, understanding structured data and generating structured output (especially JSON).",
      "grok-2-vision-1212": "This model has improvements in accuracy, instruction following and multilingual capabilities.",
      "qwen/qwen-2-vl-72b-instruct": "Qwen2-VL is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks including MathVista, DocVQA, RealWorldQA and MTVQA. Qwen2-VL can process videos over 20 minutes long for high-quality video-based Q&A, dialogue and content creation. It also has complex reasoning and decision-making capabilities, can integrate with mobile devices and robots, performing automatic operations based on visual environment and text instructions. Besides English and Chinese, Qwen2-VL now also supports understanding text in different languages in images, including most European languages, Japanese, Korean, Arabic and Vietnamese",
      "gpt-4-0125-preview": "The latest GPT-4 Turbo model has visual capabilities. Now, visual requests can use JSON mode and function calling. GPT-4 Turbo is a powerful version providing cost-effective support for multimodal tasks. It finds a balance between accuracy and efficiency, suitable for application scenarios requiring real-time interaction.",
      "google/gemini-2.0-flash-exp:free": "Gemini 2.0 Flash Experimental is Google's latest experimental multimodal AI model, showing certain quality improvements compared to historical versions, especially for world knowledge, code and long context.",
      "cohere-command-r": "Command R is a scalable generation model designed for RAG and tool use, enabling enterprises to implement production-grade AI.",
      "360gpt2-o1": "360gpt2-o1 uses tree search to build thought chains, and introduces a reflection mechanism, trained using reinforcement learning, the model has self-reflection and error correction capabilities.",
      "qwen-math-plus-latest": "Tongyi Qianwen math model is a language model specifically designed for mathematical problem solving.",
      "ernie-x1-turbo-32k": "Compared to ERNIE-X1-32K, the model effect and performance are better.",
      "hunyuan-standard-256K": "Adopts better routing strategy while mitigating load balancing and expert specialization issues. In terms of long text, large model alignment indicators reached 99.9%. MOE-256K further breaks through in length and effect, greatly expanding input length.",
      "4.0Ultra": "Spark Ultra is the most powerful version in the Spark large model series, improving understanding and summarization capabilities of multi-text content while upgrading online search links. It is a comprehensive solution for improving office productivity and accurately responding to needs, a leading intelligent product in the industry.",
      "Pro/THUDM/glm-4-9b-chat": "GLM-4-9B-Chat is the open source version among Zhipu AI's GLM-4 series pre-trained models. This model performs excellently in multiple aspects including semantics, mathematics, reasoning, code and knowledge. In addition to supporting multi-turn dialogue, GLM-4-9B-Chat also has advanced features such as web browsing, code execution, custom function calling and long text processing. The model supports 26 languages including Chinese, English, Japanese, Korean and German. In multiple benchmark tests, GLM-4-9B-Chat demonstrated excellent performance, such as AlignBench-v2, MT-Bench, MMLU and C-Eval. This model supports up to 128K context length, suitable for scientific research and commercial applications",
      "accounts/fireworks/models/mixtral-8x7b-instruct": "Mixtral MoE 8x7B Instruct is the instruction fine-tuned version of Mixtral MoE 8x7B, with chat completion API enabled.",
      "qwen-vl-v1": "Initialized from Qwen-7B language model, adding image model, pre-trained model with 448 image input resolution.",
      "dall-e-2": "Second generation DALL路E model, supporting more realistic and accurate image generation, with resolution 4 times that of the first generation",
      "yi-medium-200k": "200K ultra-long context window, providing long text deep understanding and generation capabilities.",
      "meta-llama-3.1-70b-instruct": "Llama 3.1 instruction-tuned text model has been optimized for multilingual dialogue use cases, performing exceptionally well on common industry benchmarks among many available open source and closed chat models.",
      "Qwen/Qwen2-VL-72B-Instruct": "Qwen2-VL is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks including MathVista, DocVQA, RealWorldQA and MTVQA. Qwen2-VL can process videos over 20 minutes long for high-quality video-based Q&A, dialogue and content creation. It also has complex reasoning and decision-making capabilities, can integrate with mobile devices and robots, performing automatic operations based on visual environment and text instructions. Besides English and Chinese, Qwen2-VL now also supports understanding text in different languages in images, including most European languages, Japanese, Korean, Arabic and Vietnamese",
      "sonar-reasoning-pro": "Advanced search product supporting search context, supporting advanced queries and follow-ups.",
      "qwen3-8b": "Qwen3 is a new generation Tongyi Qianwen large model with greatly enhanced capabilities, achieving industry-leading performance in multiple core capabilities such as reasoning, general purpose, Agent and multilingual, and supports thinking mode switching.",
      "sonar-pro": "Advanced search product supporting search context, supporting advanced queries and follow-ups.",
      "deepseek-r1-distill-qianfan-llama-70b": "First released on March 14, 2025, developed by the Qianfan large model research team using Llama3_70B as the base model (Built with Meta Llama), with Qianfan's corpus also added to the distillation data.",
      "yi-large-turbo": "Ultra-high cost performance, superior performance. Balanced high-precision tuning based on performance and inference speed, cost.",
      "Llama-3.2-90B-Vision-Instruct": "Advanced image reasoning capabilities suitable for visual understanding agent applications.",
      "meta-llama/Llama-3-8b-chat-hf": "Llama 3 8B Instruct Reference provides multilingual support, covering rich domain knowledge.",
      "solar-pro": "Solar Pro is a high-intelligence LLM launched by Upstage, focusing on instruction following capabilities on CPU, scoring over 80 on IFEval. Currently supports English, with official version planned for release in November 2024, which will expand language support and context length.",
      "qwq_32b": "Medium-scale reasoning model in the Qwen series. Compared to traditional instruction-tuned models, QwQ with thinking and reasoning capabilities can significantly improve performance in downstream tasks, especially when solving difficult problems.",
      "deepseek-coder-v2:236b": "DeepSeek Coder V2 is an open-source mixture-of-experts code model, performing excellently in code tasks, comparable to GPT4-Turbo.",
      "llava-v1.5-7b-4096-preview": "LLaVA 1.5 7B provides visual processing capability integration, generating complex output through visual information input.",
      "openai/gpt-4.1-nano": "GPT-4.1 nano is the fastest, most cost-effective GPT-4.1 model.",
      "meta-llama/Llama-Vision-Free": "LLaMA 3.2 aims to handle tasks combining visual and textual data. It excels in image description and visual question answering tasks, bridging the gap between language generation and visual reasoning.",
      "openai/o1-mini": "o1-mini is a fast, economical and efficient inference model designed for programming, mathematics and scientific application scenarios. This model has 128K context and knowledge cutoff date of October 2023.",
      "codestral": "Codestral is Mistral AI's first code model, providing superior support for code generation tasks.",
      "accounts/fireworks/models/llama-v3p1-70b-instruct": "Meta Llama 3.1 series is a collection of language large language models (LLM) including pre-trained and instruction fine-tuned generation models at 8B, 70B and 405B parameter scales. The Llama 3.1 instruction fine-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue applications and outperform many existing open source and closed source chat models on common industry benchmark tests.",
      "thudm/glm-z1-9b:free": "GLM-Z1-9B-0414 is a 9B parameter language model in the GLM-4 series developed by THUDM. It uses techniques initially applied to larger GLM-Z1 models, including expanded reinforcement learning, pair-wise ranking alignment, and training on reasoning-dense tasks like mathematics, coding, and logic. Despite its smaller scale, it demonstrates powerful performance in general reasoning tasks and outperforms many open-source models in its weight class.",
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": "Llama 3.1 8B model uses FP8 quantization, supports up to 131,072 context tokens, is comparable to open-source models, suitable for complex tasks, performing excellently on many industry benchmarks.",
      "glm-4-airx": "GLM-4-AirX provides a high-efficiency version of GLM-4-Air, with inference speed up to 6.6 times faster.",
      "qwen2.5:1.5b": "Qwen2.5 is Alibaba's new generation large-scale language model, supporting diverse application needs with superior performance.",
      "llava": "LLaVA is a multimodal model combining visual encoders and Vicuna for powerful visual and language understanding.",
      "qwq-plus-latest": "QwQ reasoning model trained based on Qwen2.5 model, greatly improving model reasoning ability through reinforcement learning. The model's core indicators in mathematics and code (AIME 24/25, LiveCodeBench) and some general indicators (IFEval, LiveBench, etc.) reach DeepSeek-R1 full version level.",
      "qwen-turbo-latest": "Tongyi Qianwen ultra-large scale language model, supporting inputs in different languages including Chinese and English.",
      "SenseChat-5-Cantonese": "Specifically designed for Hong Kong region's conversation habits, Cantonese and local knowledge, surpassing GPT-4 in Cantonese dialogue understanding, and comparable to GPT-4 Turbo in knowledge, reasoning, mathematics and code writing across multiple domains.",
      "step-1v-32k": "Supports visual input, enhancing multimodal interaction experience.",
      "qwen2.5-vl-72b-instruct": "Instruction following, mathematics, problem-solving, and code overall improved, universal object recognition capability enhanced, supports accurate positioning of visual elements in multiple formats, supports understanding of long video files (up to 10 minutes) and event timestamp positioning at second-level, can understand temporal order and speed, based on analysis and positioning capabilities supports OS or Mobile Agent, strong key information extraction capability and Json format output capability, this version is 72B version, the most capable version in this series.",
      "deepseek-v2:236b": "DeepSeek V2 236B is DeepSeek's design code model, providing powerful code generation capabilities.",
      "command-light": "A smaller, faster version of Command, almost as powerful but faster.",
      "Qwen2.5-7B-Instruct": "Tongyi Qianwen 2.5's open-source 7B scale model.",
      "ERNIE-3.5-8K": "Baidu's self-developed flagship large language model, covering massive Chinese-English corpus, possessing powerful general capabilities, can meet most dialogue Q&A, creative generation, and plugin application scenario demands; supports automatic connection to Baidu search plugin, ensuring Q&A information timeliness.",
      "gemini-2.5-pro-preview-03-25": "Gemini 2.5 Pro Preview is Google's most advanced thinking model, capable of reasoning about complex problems in code, mathematics and STEM fields, as well as using long context to analyze large datasets, code repositories and documents.",
      "hunyuan-code": "Hunyuan's latest code generation model, trained on 200B high-quality code data base model, iterated with half-year high-quality SFT data training, context window length expanded to 8K, ranked top in five language code generation automatic evaluation metrics; performance ranks first in five language 10-item comprehensive code task human high-quality evaluation",
      "command-r-plus": "Command R+ is a high-performance large language model designed specifically for real enterprise scenarios and complex applications.",
      "qwen2:0.5b": "Qwen2 is Alibaba's new generation large-scale language model, supporting diverse application needs with superior performance.",
      "grok-2-1212": "This model has improvements in accuracy, instruction following and multilingual capabilities.",
      "open-mixtral-8x22b": "Mixtral 8x22B is a larger expert model, focusing on complex tasks, providing excellent reasoning capabilities and higher throughput.",
      "Qwen2-72B-Instruct": "Qwen2 is the new generation large language model series launched by the Qwen team. It is based on the Transformer architecture and employs SwiGLU activation function, attention QKV bias, group query attention, mixture of sliding window attention and full attention. Additionally, the Qwen team has improved the tokenizer to adapt to multiple natural languages and code.",
      "gpt-3.5-turbo-instruct": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
      "Qwen2.5-72B-Instruct": "LLM oriented towards Chinese and English, focusing on language, programming, mathematics, reasoning and other domains.",
      "gemini-2.0-flash-preview-image-generation": "Gemini 2.0 Flash preview model, supporting image generation",
      "Baichuan3-Turbo": "Optimized for enterprise high-end scenarios, greatly improved effectiveness, high cost-performance ratio. Compared to Baichuan2 model, content creation improved by 30%, knowledge Q&A improved by 37%, role-playing ability improved by 40%. Overall performance better than GPT3.5.",
      "taichu_vl": "Integrates image understanding, knowledge migration, logical reasoning and other capabilities, showing outstanding performance in image-text Q&A domain",
      "openrouter/auto": "Based on context length, topic and complexity, your request will be routed to Llama 3 70B Instruct, Claude 3.5 Sonnet (self-adjusting) or GPT-4o.",
      "thudm/glm-4-9b-chat": "GLM-4-9B-Chat is the open source version among Zhipu AI's GLM-4 series pre-trained models. This model performs excellently in multiple aspects including semantics, mathematics, reasoning, code and knowledge. In addition to supporting multi-turn dialogue, GLM-4-9B-Chat also has advanced features such as web browsing, code execution, custom function calling and long text processing. The model supports 26 languages including Chinese, English, Japanese, Korean and German. In multiple benchmark tests, GLM-4-9B-Chat demonstrated excellent performance, such as AlignBench-v2, MT-Bench, MMLU and C-Eval. This model supports up to 128K context length, suitable for scientific research and commercial applications",
      "moonshot-v1-8k": "Moonshot V1 8K specially designed for short text generation tasks, has efficient processing capabilities, can process 8,192 tokens, very suitable for brief conversations, speed reading and quick content generation.",
      "gpt-4": "GPT-4 provides a larger context window, capable of processing longer text input, suitable for scenarios requiring extensive information integration and data analysis.",
      "ernie-tiny-8k": "ERNIE Tiny is Baidu's self-developed ultra-high-performance large language model, with the lowest deployment and fine-tuning costs among the Wenxin series models.",
      "Qwen/Qwen2.5-7B-Instruct-Turbo": "Qwen2.5 is a brand new large language model series aimed at optimizing instruction-based task processing.",
      "deepseek-ai/deepseek-r1": "DeepSeek-R1 series optimizes reasoning performance through reinforcement learning and cold-start data, open-source model refreshing task benchmarks, surpassing OpenAI-o1-mini level.",
      "solar-mini-ja": "Solar Mini (Ja) extends Solar Mini's capabilities, focusing on Japanese while maintaining high efficiency and superior performance in English and Korean usage.",
      "codestral-latest": "Codestral is our most advanced coding language model, second version released in March 2025, specialized in low-latency, high-frequency task intermediate filling (RST), code correction and test generation.",
      "meta-llama/llama-3.1-8b-instruct": "Meta's latest generation Llama 3.1 model series (8B, 70 billion parameters) instruction fine-tuned version is particularly fast and efficient. In industry evaluations, it shows strong performance, surpassing many leading closed-source models. (Only available to enterprise real-name verified subjects)",
      "Meta-Llama-3.2-1B-Instruct": "Advanced cutting-edge small language model with language understanding, superior reasoning capabilities and text generation abilities.",
      "minicpm-v": "MiniCPM-V is OpenBMB's new generation multimodal large model, featuring superior OCR recognition and multimodal understanding capabilities, supporting broad application scenarios.",
      "google/gemma-2b-it": "Gemma Instruct (2B) provides basic instruction processing capabilities, suitable for lightweight applications.",
      "gpt-3.5-turbo-1106": "GPT 3.5 Turbo, suitable for various text generation and understanding tasks, Currently points to gpt-3.5-turbo-0125",
      "glm-4-flash": "GLM-4-Flash is the ideal choice for handling simple tasks, fastest speed and free.",
      "anthropic/claude-3.5-haiku": "Claude 3.5 Haiku is Anthropic's fastest next-generation model. Compared to Claude 3 Haiku, Claude 3.5 Haiku has improvements in all technical aspects and surpasses the previous generation's largest model Claude 3 Opus in many intelligence benchmark tests.",
      "qwen/qwen-2-7b-instruct:free": "Qwen2 is a brand new large language model series with stronger understanding and generation capabilities.",
      "accounts/fireworks/models/llama-v3-8b-instruct": "Meta has developed and released the Meta Llama 3 series of large language models (LLM), which is a collection of pre-trained and instruction fine-tuned text generation models at 8B and 70B parameter scales. The Llama 3 instruction-tuned model is optimized for dialogue application scenarios and outperforms many existing open-source chat models in common industry benchmark tests.",
      "accounts/fireworks/models/llama-v3-8b-instruct-hf": "Meta Llama 3 instruction-tuned model is optimized for dialogue application scenarios and outperforms many existing open-source chat models in common industry benchmark tests. Llama 3 8B Instruct (HF version) is the original FP16 version of Llama 3 8B Instruct, and its results should match the official Hugging Face implementation.",
      "gpt-4o": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education and technical support.",
      "llama-3.2-vision-instruct": "Llama 3.2-Vision instruction-tuned model has been optimized for visual recognition, image reasoning, image description and answering common questions related to images.",
      "sonar-deep-research": "Deep Research conducts comprehensive expert-level research and synthesizes it into accessible, actionable reports.",
      "claude-3-opus-20240229": "Claude 3 Opus is Anthropic's most powerful model for handling highly complex tasks. It demonstrates superior performance in capability, intelligence, fluency and understanding.",
      "amazon.titan-text-premier-v1:0": "Titan Text Premier is a powerful advanced model in the Titan Text series, aimed at providing superior performance for broad enterprise applications. Leveraging its cutting-edge capabilities, it offers higher accuracy and excellent results, making it the perfect choice for organizations requiring first-class text processing solutions.",
      "deepseek-r1-70b-fast-online": "DeepSeek R1 70B fast version, supporting real-time internet search, providing faster response speed while maintaining model performance.",
      "deepseek-ai/DeepSeek-V2.5": "DeepSeek-V2.5 is an upgraded version of DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating the general and coding capabilities of the two previous versions. The model has been optimized in multiple aspects, including writing and instruction following capabilities, better aligning with human preferences. DeepSeek-V2.5 has achieved significant improvements on various evaluation benchmarks such as AlpacaEval 2.0, ArenaHard, AlignBench and MT-Bench.",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": "DeepSeek-R1 distillation model, optimizing reasoning performance through reinforcement learning and cold-start data, open-source model refreshing task benchmarks.",
      "llava:34b": "LLaVA is a multimodal model combining visual encoders and Vicuna for powerful visual and language understanding.",
      "Skylark2-pro-character-4k": "Second generation cloud finch (Skylark) model, Skylark2-pro-character model has excellent character role-playing and chat capabilities, excelling at performing different roles and engaging in conversations with users based on user prompts, with distinctive character styles, natural and fluent dialogue content, suitable for building chatbots, virtual assistants and online customer service scenarios, with relatively high response speed.",
      "deepseek_r1_distill_llama_70b": "DeepSeek-R1-Distill-Llama-70B is a model obtained by distillation training based on Llama-3.3-70B-Instruct. This model is part of the DeepSeek-R1 series, fine-tuned using samples generated by DeepSeek-R1, demonstrating excellent performance in multiple domains such as mathematics, programming and reasoning.",
      "Qwen2.5-32B-Instruct": "Tongyi Qianwen 2.5's open-source 32B scale model.",
      "glm-4-0520": "GLM-4-0520 is the latest model version, designed for highly complex and diversified tasks, with excellent performance.",
      "MiniMax-Text-01": "In the MiniMax-01 series models, we made bold innovations: the first large-scale implementation of linear attention mechanism, traditional Transformer architecture is no longer the only choice. This model has 560 billion parameters, with 59 billion single activations. The model's comprehensive performance surpasses top ocean models, while being able to efficiently process global context up to 100K tokens, 2x that of GPT-4 and 10x that of Claude-3.5-Sonnet.",
      "gemini-1.5-pro-exp-0827": "Gemini 1.5 Pro 0827 combines the latest optimization techniques, bringing more efficient multimodal data processing capabilities.",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "Mixtral-8x22B Instruct (141B) is a super large language model that supports extremely high processing demands.",
      "DeepSeek-R1-Distill-Qwen-1.5B": "DeepSeek-R1-Distill-Qwen-1.5B is a distilled model based on Qwen-2.5 series by DeepSeek-R1.",
      "THUDM/GLM-Z1-9B-0414": "GLM-Z1-9B-0414 is a small model in the GLM series with only 90 billion parameters, but maintains open-source tradition while showing amazing capabilities. Despite its relatively small size, the model performs excellently in mathematical reasoning and general tasks, with its overall performance already at the forefront among open-source models of similar scale.",
      "o1-mini": "o1-mini is a fast, economically efficient inference model designed for programming, mathematics, and scientific application scenarios. This model has 128K context and knowledge cutoff date of October 2023.",
      "ernie-4.5-turbo-128k": "Wenxin 4.5 Turbo also shows significant improvements in hallucination reduction, logical reasoning, and coding capabilities. Compared to Wenxin 4.5, it's faster and cheaper. With comprehensive model capability improvements, it better meets multi-round long history dialogue processing and long document understanding question-answering tasks.",
      "qwen2.5-vl-instruct": "Qwen2.5-VL is the latest version of the vision-language model in the Qwen model family.",
      "moonshot-v1-128k-vision-preview": "Kimi vision models (including moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview etc.) can understand image content, including image text, image colors, and object shapes.",
      "anthropic.claude-3-opus-20240229-v1:0": "Claude 3 Opus is Anthropic's most powerful AI model, with state-of-the-art capabilities on highly complex tasks. It can handle open-ended prompts and novel scenarios, demonstrating excellent fluency and human-like understanding. Claude 3 Opus represents the frontier of AI possibilities. Claude 3 Opus can process images and return text outputs, with a 200K context window.",
      "llama3-8b-8192": "Meta Llama 3 8B brings high-quality inference performance, suitable for multi-scenario application needs.",
      "command": "A dialogue model that follows instructions, demonstrating high quality and reliability in language tasks, and compared to our base generation models, has longer context length.",
      "DeepSeek-R1-Distill-Qwen-7B": "DeepSeek-R1-Distill-Qwen-7B is a distilled model based on Qwen-2.5 series by DeepSeek-R1.",
      "Qwen/Qwen2.5-Coder-32B-Instruct": "Qwen2.5 Coder 32B Instruct is the latest version of Alibaba Cloud's code-specific large language model series. This model has significantly improved code generation, inference, and repair capabilities through 55 billion tokens of training on the Qwen2.5 base. It not only enhanced coding capabilities but maintained advantages in mathematics and general capabilities. The model provides a more comprehensive foundation for code intelligence bodies and other practical applications",
      "meta-llama/llama-3.2-90b-vision-instruct": "LLaMA 3.2 aims to handle tasks combining vision and text data. It performs excellently in image description and visual question answering tasks, bridging the gap between language generation and visual inference.",
      "deepseek/deepseek-r1:free": "DeepSeek-R1 greatly enhances model inference capabilities with minimal annotated data. Before outputting the final answer, the model first outputs a chain of thought content to improve the accuracy of the final answer.",
      "accounts/fireworks/models/mythomax-l2-13b": "An improved version of MythoMix, possibly its more refined variant, is a merger of MythoLogic-L2 and Huginn, using highly experimental tensor-type merging techniques. Due to its unique properties, this model performs excellently in storytelling and character roleplay aspects.",
      "deepseek/deepseek-v3/community": "DeepSeek-V3 achieved major breakthroughs in inference speed compared to previous models. Ranking first among open-source models and competing with the world's most advanced closed-source models. DeepSeek-V3 adopts multi-head attention (MLA) and DeepSeekMoE architecture, which were fully validated in DeepSeek-V2. Additionally, DeepSeek-V3 pioneered an auxiliary lossless strategy for load balancing and set multi-target pre-training objectives to achieve stronger performance.",
      "gemini-2.0-flash-lite": "Gemini 2.0 Flash model variant, optimized for cost-effectiveness and low latency objectives.",
      "SenseNova-V6-Pro": "Achieves native unification of image, text, and video capabilities, breaking through traditional multi-modal separation limitations, winning dual championships in OpenCompass and SuperCLUE evaluations.",
      "ernie-x1-32k": "Possesses stronger understanding, planning, reflection, and evolution capabilities. As a more comprehensive deep thinking model, Wenxin X1 combines accuracy, creativity, and elegance, performing exceptionally well in Chinese knowledge Q&A, literary creation, article writing, daily conversation, logical reasoning, complex calculations, and tool calling.",
      "gemini-2.0-flash-lite-001": "Gemini 2.0 Flash model variant, optimized for cost-effectiveness and low latency objectives.",
      "charglm-3": "CharGLM-3 is specially designed for character roleplay and emotional companionship, supporting ultra-long multi-round memory and personalized dialogue, with broad applications.",
      "openai/o3-mini-high": "o3-mini high inference grade version, providing higher intelligence under the same cost and latency targets as o1-mini.",
      "gpt-4.1-nano": "GPT-4.1 nano is the fastest, most cost-effective GPT-4.1 model.",
      "Doubao-lite-128k": "Has extremely fast response speed, better price-performance ratio, providing more flexible choices for customer different scenarios. Supports 128k context window inference and fine-tuning.",
      "qwen/qwen3-30b-a3b:free": "Qwen3 is the latest generation of the Qwen large language model series, featuring dense and Mixture of Experts (MoE) architecture, performing excellently in inference, multilingual support, and advanced agency tasks. Its unique ability to seamlessly switch between complex reasoning thinking mode and efficient dialogue non-thinking mode ensures multi-functional, high-quality performance.\n\nQwen3 significantly outperforms previous models like QwQ and Qwen2.5, providing superior mathematics, coding, common sense reasoning, creative writing, and interactive dialogue capabilities. Qwen3-30B-A3B variant contains 305B parameters (3B activated parameters), 28 layers, 128 experts (8 activated per task), and supports up to 131K token context (using YaRN), setting a new standard for open-source models.",
      "Pro/Qwen/Qwen2.5-7B-Instruct": "Qwen2.5-7B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 7B model shows significant improvements in coding and mathematical domains. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows notable improvements in instruction following, understanding structured data, and generating structured output (especially JSON)",
      "accounts/fireworks/models/llama-v3p2-3b-instruct": "Llama 3.2 3B Instruct is Meta's lightweight multilingual model. The model is designed for high-efficiency operation, offering significant latency and cost advantages compared to larger models. Its typical use cases include query and prompt rewriting, as well as writing assistance.",
      "Phi-3-small-128k-instruct": "Same Phi-3-small model but with larger context size, suitable for RAG or few-shot prompting.",
      "meta/llama-3.1-405b-instruct": "Advanced LLM supporting synthetic data generation, knowledge distillation and inference, suitable for chatbots, coding and domain-specific tasks.",
      "deepseek-ai/deepseek-vl2": "DeepSeek-VL2 is a Mixture of Experts (MoE) vision-language model developed based on DeepSeekMoE-27B, adopting sparse activation MoE architecture to achieve superior performance while only activating 4.5B parameters. The model performs exceptionally in visual Q&A, optical character recognition, document table/chart understanding, and visual localization tasks.",
      "gemini-1.5-pro-001": "Gemini 1.5 Pro 001 is a scalable multimodal AI solution supporting a wide range of complex tasks.",
      "qwen/qwen3-32b:free": "Qwen3-32B is a dense 328B parameter foundation language model in the Qwen3 series, optimized for complex reasoning and efficient dialogue. It supports seamless switching between a 'thinking' mode for mathematics, coding, and logical reasoning tasks and a 'non-thinking' mode for faster, general conversation. The model demonstrates powerful capabilities in instruction following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively processes 32K token context and can extend to 131K tokens using YaRN-based extension.",
      "glm-4v-flash": "GLM-4V-Flash focuses on efficient single image understanding, suitable for scenarios requiring quick image analysis, such as real-time image analysis or batch image processing.",
      "Phi-3-mini-4k-instruct": "The smallest member of the Phi-3 family, optimized for quality and low latency.",
      "THUDM/GLM-4-9B-0414": "GLM-4-9B-0414 is a small model in the GLM series with 90 billion parameters. This model inherits the technical features of the GLM-4-32B series but provides a more lightweight deployment option. Despite its smaller size, GLM-4-9B-0414 still demonstrates excellent capabilities in code generation, webpage design, SVG graphic generation, and search-based writing tasks.",
      "yi-large-preview": "Early version, recommend using yi-large (new version).",
      "deepseek-v3-0324": "DeepSeek-V3-0324 is a 171B parameter MoE model with outstanding advantages in programming and technical capabilities, context understanding and long text processing.",
      "Skylark2-pro-turbo-8k": "Second generation Skylark model, Skylark2-pro-turbo-8k offers faster inference, lower cost, with an 8k context window length.",
      "moonshot-v1-8k-vision-preview": "Kimi vision models (including moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview etc.) can understand image content, including image text, image colors, and object shapes.",
      "qwen-plus": "Tongyi Qianwen super large-scale language model enhanced version, supporting input in different languages including Chinese and English.",
      "gemini-1.5-flash-8b-latest": "Gemini 1.5 Flash 8B is an efficient multimodal model supporting extensive application extensions.",
      "gpt-4o-realtime-preview-2024-10-01": "GPT-4o real-time version, supporting audio and text real-time input and output",
      "nousresearch/hermes-2-pro-llama-3-8b": "Hermes 2 Pro Llama 3 8B is an upgraded version of Nous Hermes 2, including the latest internally developed datasets.",
      "step-1o-turbo-vision": "This model has powerful image understanding capabilities and is stronger than 1o in mathematics and coding domains. The model is smaller than 1o and outputs faster.",
      "accounts/fireworks/models/qwen2p5-coder-32b-instruct": "Qwen2.5-Coder is the latest generation of Qwen large language models designed specifically for code (formerly known as CodeQwen). Note: This model is currently being provided experimentally as a serverless model. If used in production environments, please note that Fireworks may discontinue deployment of this model in a short time.",
      "anthropic/claude-3-haiku": "Claude 3 Haiku is Anthropic's fastest and most cost-efficient model, aimed at achieving near-instantaneous response. It has fast and accurate directional capabilities.",
      "gemini-1.0-pro-latest": "Gemini 1.0 Pro is Google's high-performance AI model, designed for broad task extension.",
      "meta-llama/Meta-Llama-3-70B-Instruct-Lite": "Llama 3 70B Instruct Lite is suitable for environments requiring high performance and low latency.",
      "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": "The Llama 3.1 70B model has been finely tuned, suitable for high-load applications, quantized to FP8 to provide more efficient computational capabilities and accuracy, ensuring superior performance in complex scenarios.",
      "THUDM/chatglm3-6b": "ChatGLM3-6B is an open-source model in the ChatGLM series, developed by Zhipu AI. The model retains the excellent characteristics of previous generations, such as smooth dialogue and low deployment threshold, while introducing new features. It employs more diverse training data, more comprehensive training epochs, and more rational training strategies, showing excellent performance among pre-trained models under 10B. ChatGLM3-6B supports multi-round dialogues, tool calling, code execution, and Agent tasks. In addition to the dialogue model, it also open-sources the base model ChatGLM-6B-Base and long-text dialogue model ChatGLM3-6B-32K. The model is completely open for academic research and also allows free commercial use after registration.",
      "moonshot-v1-128k": "Moonshot V1 128K is a model with ultra-long context processing capabilities, suitable for generating super-long text, meeting complex generation task requirements, able to process content up to 128,000 tokens, particularly suitable for scientific research, academic and large document generation scenarios.",
      "accounts/fireworks/models/qwen-qwq-32b-preview": "Qwen QwQ model focuses on promoting AI reasoning, and demonstrates the power of open models to match closed-source frontier models in reasoning capabilities. QwQ-32B-Preview is an experimental release version, performing comparably to o1 in analysis and reasoning capabilities on GPQA, AIME, MATH-500, and LiveCodeBench benchmarks, surpassing GPT-4o and Claude 3.5 Sonnet. Note: This model is currently being provided experimentally as a serverless model. If used in production environments, please note that Fireworks may discontinue deployment of this model in a short time.",
      "ernie-lite-8k": "ERNIE Lite is Baidu's self-developed lightweight large language model, combining excellent model effects with inference capabilities, suitable for low-compute AI accelerator card inference use.",
      "glm-zero-preview": "GLM-Zero-Preview has strong complex reasoning capabilities, showing excellence in logical reasoning, mathematics, programming, and other domains.",
      "Meta-Llama-3.3-70B-Instruct": "Llama 3.3 is the most advanced multilingual open-source large language model in the Llama series, experiencing the performance of 405B models at extremely low cost. Based on Transformer structure and enhanced through supervised fine-tuning (SFT) and human feedback reinforcement learning (RLHF) to improve usefulness and safety. Its instruction-tuned version is optimized for multilingual dialogue, performing better than many open-source and closed chat models on multiple industry benchmarks. Knowledge cutoff date is December 2023.",
      "meta-llama/Meta-Llama-3.1-70B": "Llama 3.1 is Meta's frontier model, supporting up to 405B parameters, applicable to complex dialogue, multilingual translation, and data analysis domains.",
      "360/deepseek-r1": "[360 deployment version] DeepSeek-R1 extensively used reinforcement learning techniques in the post-training phase, greatly enhancing model inference capabilities with minimal annotated data. In mathematics, coding, natural language reasoning, and other tasks, its performance matches OpenAI o1 official version.",
      "anthropic.claude-3-5-sonnet-20241022-v2:0": "Claude 3.5 Sonnet raises industry standards, performing better than competitor models and Claude 3 Opus, showing excellence in broad evaluations while having our mid-tier models' speed and cost.",
      "deepseek/deepseek-r1-distill-qwen-32b": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, trained using outputs from DeepSeek R1. This model surpassed OpenAI's o1-mini in multiple benchmark tests, achieving state-of-the-art results for dense models. Here are some benchmark test results:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nThe model, fine-tuned using outputs from DeepSeek R1, demonstrates competitive performance comparable to larger frontier models.",
      "internlm2.5-latest": "Our still-maintained old version model, after multiple iterations has extremely excellent and stable performance, including 7B, 20B multiple model parameter options, supporting 1M context length and stronger instruction following and tool calling capabilities. By default points to our latest released InternLM2.5 series models, currently pointing to internlm2.5-20b-chat.",
      "emohaa": "Emohaa is a psychological model with professional counseling capabilities, helping users understand emotional issues.",
      "qwen3-235b-a22b": "Qwen3 is a new generation of Tongyi Qianwen large model with greatly improved capabilities, reaching industry-leading levels in multiple core capabilities such as reasoning, general, Agent and multilingual support, and supporting thinking mode switching.",
      "qwen2.5-vl-32b-instruct": "Qwen2.5VL series model, achieving performance close to Qwen2.5VL-72B in math and science problem solving, with response style heavily adjusted towards human preference, especially for objective queries like mathematics, logical reasoning, and knowledge Q&A, with noticeably improved model response practicality and format clarity. This is the 32B version.",
      "ERNIE-Lite-Pro-128K": "Baidu's self-developed lightweight large language model, combining excellent model effects with inference capabilities, performing better than ERNIE Lite, suitable for low-compute AI accelerator card inference use.",
      "mistral-nemo-instruct": "Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is the instruction-tuned version of Mistral-Nemo-Base-2407.",
      "gpt-35-turbo": "GPT 3.5 Turbo, OpenAI's efficient model suitable for chat and text generation tasks, supporting parallel function calling.",
      "llama-2-7b-chat": "Llama2 is a series of large language models (LLMs) developed and open-sourced by Meta, a group of generative text models ranging from 70B to 700B parameters that have been pre-trained and fine-tuned. Architecturally, LLama2 is an auto-regressive language model using an optimized transformer architecture. The adjusted versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for usefulness and safety. Llama2 has more unrelenting performance than the Llama series on various benchmark datasets, providing design and development ideas for many other models.",
      "Qwen/Qwen2-7B-Instruct": "Qwen2-7B-Instruct is the instruction-tuned large language model in the Qwen2 series with 7B parameters. This model is based on the Transformer architecture and employs techniques such as SwiGLU activation function, attention QKV bias, and group-query attention. It can handle large-scale inputs. The model shows excellent performance in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning across multiple benchmark tests, surpassing most open-source models and showing competitive capabilities comparable to proprietary models in certain tasks. Qwen2-7B-Instruct outperforms Qwen1.5-7B-Chat in multiple evaluations, showing significant performance improvements",
      "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) is a high-precision instruction model suitable for complex calculations.",
      "Phi-3.5-mini-instruct": "Updated version of the Phi-3-mini model.",
      "qwen/qwen-2.5-72b-instruct": "Qwen2.5-72B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 72B model shows significant improvements in coding and mathematical domains. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows notable improvements in instruction following, understanding structured data, and generating structured output (especially JSON).",
      "Qwen/QwQ-32B": "QwQ is the reasoning model of the Qwen series. Compared to traditional instruction-tuned models, QwQ has thinking and reasoning capabilities, able to achieve significantly enhanced performance in downstream tasks, especially in solving difficult problems. QwQ-32B is a medium-sized reasoning model that can achieve competitive performance in comparison with the most advanced reasoning models (such as DeepSeek-R1, o1-mini). This model employs techniques such as RoPE, SwiGLU, RMSNorm, and Attention QKV bias, featuring a 64-layer network structure and 40 Q attention heads (28 KV in MQA architecture).",
      "deepseek-coder-v2": "DeepSeek Coder V2 is an open-source mixture-of-experts code model, performing excellently in code tasks, comparable to GPT4-Turbo.",
      "llama-3.1-instruct": "Llama 3.1 instruction-tuned model has been optimized for dialogue scenarios, surpassing many existing open-source chat models in common industry benchmark tests.",
      "gemini-1.5-flash-latest": "Gemini 1.5 Flash is Google's latest multimodal AI model with fast processing capabilities, supporting text, image, and video input, suitable for high-efficiency extensions of various tasks.",
      "hunyuan-pro": "Billion-parameter scale MOE-32K long text model. Achieves absolute frontier level in various benchmarks, complex instructions and reasoning, possesses complex mathematical capabilities, supports functioncall, with focused optimization in multilingual translation, finance, legal, medical, and other domain applications.",
      "microsoft/wizardlm-2-8x22b": "WizardLM 2 is a language model provided by Microsoft AI, performing particularly well in complex dialogue, multilingual, reasoning, and intelligent assistant domains.",
      "grok-3-beta": "Flagship model, excelling at data extraction, programming, and text summarization for enterprise applications, with deep knowledge in finance, medical, legal, and scientific domains.",
      "qwen2.5-coder-7b-instruct": "Tongyi Qianwen code model open-source version.",
      "glm-4-flash-250414": "GLM-4-Flash is the ideal choice for processing simple tasks, being the fastest and free.",
      "doubao-1.5-vision-lite": "Doubao-1.5-vision-lite is a newly upgraded multimodal large model, supporting image recognition at any resolution and extreme aspect ratios, strengthening visual reasoning, document recognition, detail information understanding, and instruction following capabilities. Supports 128k context window, with output length supporting up to 16k tokens.",
      "Doubao-lite-32k": "Has extremely fast response speed, better price-performance ratio, providing more flexible choices for customer different scenarios. Supports 32k context window inference and fine-tuning.",
      "command-a-03-2025": "Command A is our strongest model to date, performing excellently in tool use, agency, retrieval-augmented generation (RAG), and multilingual application scenarios. Command A has a 256K context length, can run with just two GPUs, and compared to Command R+ 08-2024, has increased throughput by 150%.",
      "codellama": "Code Llama is an LLM specialized in code generation and discussion, combining broad programming language support, suitable for developer environments.",
      "qwen/qwen-2-7b-instruct": "Qwen2 is the completely new Qwen large language model series. Qwen2 7B is a transformer-based model, showing excellence in language understanding, multilingual capabilities, programming, mathematics, and reasoning.",
      "llama-3.3-70b-instruct": "Meta's released LLaMA 3.3 multilingual large language model (LLMs) is a generative model trained and instruction-tuned providing 70B scale (text input & text output). The model was trained on over 15T of data, supporting English, German, French, Italian, Portuguese, Hindi, Spanish and Thai, with knowledge updated until December 2023.",
      "gemini-1.5-pro-latest": "Gemini 1.5 Pro supports up to 2 million tokens, is the ideal choice for medium-sized multimodal models, suitable for multi-aspect support of complex tasks.",
      "gpt-4-turbo": "The latest GPT-4 Turbo model has visual capabilities. Now, visual requests can use JSON mode and function calling. GPT-4 Turbo is a powerful version providing cost-effective support for multimodal tasks. It finds a balance between accuracy and efficiency, suitable for application scenarios requiring real-time interaction.",
      "gpt-4-0613": "GPT-4 provides a larger context window, able to process longer text input, suitable for scenarios requiring broad information integration and data analysis.",
      "tts-1-hd": "Latest text-to-speech model, optimized for quality",
      "llama-3.2-11b-vision-preview": "LLaMA 3.2 aims to handle tasks combining vision and text data. It performs excellently in image description and visual question answering tasks, bridging the gap between language generation and visual inference.",
      "google/gemma-2-9b-it": "Gemma 2 9B developed by Google, providing efficient instruction response and comprehensive capabilities.",
      "gpt-4o-mini-tts": "GPT-4o mini TTS is a text-to-speech model built on GPT-4o mini, which is a fast and powerful language model. Using it can convert text into naturally sounding voice text. Maximum input token count is 2000.",
      "mixtral-8x7b-32768": "Mixtral 8x7B provides high-reliability parallel computing capabilities, suitable for complex tasks.",
      "ernie-4.5-8k-preview": "Wenxin large model 4.5 is Baidu's self-developed new generation native multimodal foundation large model, achieving collaborative optimization through multi-modal joint modeling, with excellent multimodal understanding capabilities; possessing more refined language capabilities, comprehensive improvement in understanding, generation, logic, and memory capabilities, with significant improvements in hallucination reduction, logical reasoning, and coding capabilities.",
      "pixtral-12b-2409": "The Pixtral model demonstrates powerful capabilities in chart and graph understanding, document Q&A, multimodal reasoning, and instruction following tasks, able to take in images at natural resolutions and aspect ratios, and can process any number of images in long context windows up to 128K tokens.",
      "anthropic.claude-3-5-sonnet-20240620-v1:0": "Claude 3.5 Sonnet raises industry standards, performing better than competitor models and Claude 3 Opus, showing excellence in broad evaluations while having our mid-tier models' speed and cost.",
      "gemini-1.0-pro-001": "Gemini 1.0 Pro 001 (Tuning) provides stable and tunable performance, being the ideal choice for complex task solutions.",
      "dall-e-3": "The latest DALL路E model, released in November 2023. Supports more realistic, accurate image generation with stronger detail expression capabilities.",
      "meta-llama/llama-3-70b-instruct": "Llama 3 70B Instruct optimized for high-quality dialogue scenarios, performing excellently in various human evaluations.",
      "mistralai/mistral-nemo": "Mistral Nemo is a 4.3B parameter model for multilingual support and high-performance programming.",
      "yi-medium": "Medium-sized model upgrade fine-tuning, with balanced capabilities and high cost-performance. Deeply optimized instruction following capabilities.",
      "Qwen/Qwen2.5-VL-72B-Instruct": "Qwen2.5-VL is the vision-language model in the Qwen2.5 series. This model has significant improvements in multiple aspects: stronger visual understanding capabilities, able to recognize common objects, analyze text, charts and layouts; as a visual agent can reason and dynamically guide tool use; supports understanding over 1-hour long videos and capturing key events; can accurately locate objects in images through generating bounding boxes or points; supports generating structured output, particularly suitable for scanning data like invoices and tables.",
      "01-ai/yi-1.5-34b-chat": "Zero One Everything, the latest open-source fine-tuned model, 34 billion parameters, fine-tuned to support multiple dialogue scenarios, high-quality training data, aligned with human preferences.",
      "qwen3-0.6b": "Qwen3 is a new generation of Tongyi Qianwen large model with greatly improved capabilities, reaching industry-leading levels in multiple core capabilities such as reasoning, general, Agent and multilingual support, and supporting thinking mode switching.",
      "abab6.5s-chat": "Suitable for broad natural language processing tasks, including text generation, dialogue systems, etc.",
      "deepseek_r1_distill_qwen_14b": "DeepSeek-R1-Distill-Qwen-14B is a model obtained through knowledge distillation based on Qwen2.5-14B. This model was fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating excellent reasoning capabilities.",
      "doubao-1.5-thinking-pro": "Doubao-1.5 brand new deep thinking model, showing outstanding performance in professional domains like mathematics, programming, scientific reasoning and general tasks like creative writing, reaching or approaching industry-leading levels in multiple authoritative benchmarks such as AIME 2024, Codeforces, GPQA. Supports 128k context window, 16k output.",
      "meta/llama-3.2-90b-vision-instruct": "Cutting-edge vision-language model, excelling at high-quality inference from images.",
      "qwen/qwen3-30b-a3b": "Qwen3 is a new generation of Tongyi Qianwen large model with greatly improved capabilities, reaching industry-leading levels in multiple core capabilities such as reasoning, general, Agent and multilingual support, and supporting thinking mode switching.",
      "qvq-72b-preview": "QVQ-72B-Preview is an experimental research model developed by the Qwen team, focusing on improving visual reasoning capabilities.",
      "codegemma:2b": "CodeGemma is a lightweight language model specialized for different programming tasks, supporting rapid iteration and integration.",
      "google/gemini-2.5-flash-preview": "Gemini 2.5 Flash is Google's most advanced flagship model, designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in 'thinking' capabilities, enabling it to provide responses with higher accuracy and nuanced context processing.\n\nNote: This model has two variants: thinking and non-thinking. Output pricing varies significantly depending on whether thinking capabilities are activated. If you choose the standard variant (without 'thinking' suffix), the model will explicitly avoid generating thinking tokens.\n\nTo utilize thinking capabilities and receive thinking tokens, you must choose the 'thinking' variant, which will produce higher thinking output pricing.\n\nAdditionally, Gemini 2.5 Flash can be configured through the 'maximum reasoning tokens' parameter, as described in the document (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
      "amazon.titan-text-lite-v1": "Amazon Titan Text Lite is a lightweight, efficient model particularly suitable for fine-tuning English tasks, including summarization and drafting, when customers want a smaller, more economical model that is also highly customizable.",
      "gemini-2.0-flash-exp-image-generation": "Gemini 2.0 Flash experimental model, supporting image generation",
      "mistral-medium-latest": "Mistral Medium 3 provides state-of-the-art performance at 1/8 the cost and fundamentally simplifies enterprise deployment.",
      "qwen3": "Qwen3 is Alibaba's new generation of large-scale language models, supporting diverse application needs with superior performance.",
      "meta-llama/Llama-2-70b-hf": "LLaMA-2 provides excellent language processing capabilities and outstanding interactive experience.",
      "codegemma": "CodeGemma is a lightweight language model specialized for different programming tasks, supporting rapid iteration and integration.",
      "SenseChat-32K": "Base version model (V4), 32K context length, flexible application to various scenarios",
      "grok-3-mini-beta": "Lightweight model, thinks before responding. Runs fast and intelligently, suitable for logical tasks that don't require deep domain knowledge, and can obtain original thinking paths.",
      "accounts/fireworks/models/mistral-small-24b-instruct-2501": "24B parameter model with state-of-the-art capabilities comparable to larger models.",
      "deepseek-reasoner": "Reasoning model launched by DeepSeek. Before outputting the final answer, the model first outputs a chain of thought content to improve the accuracy of the final answer.",
      "SenseChat-5": "Latest version model (V5.5), 128K context length, with significantly improved capabilities in mathematical reasoning, English conversation, instruction following, and long text understanding, surpassing GPT-4o.",
      "google/gemini-2.5-flash-preview:thinking": "Gemini 2.5 Flash is Google's most advanced flagship model, designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in 'thinking' capabilities, enabling it to provide responses with higher accuracy and nuanced context processing.\n\nNote: This model has two variants: thinking and non-thinking. Output pricing varies significantly depending on whether thinking capabilities are activated. If you choose the standard variant (without 'thinking' suffix), the model will explicitly avoid generating thinking tokens.\n\nTo utilize thinking capabilities and receive thinking tokens, you must choose the 'thinking' variant, which will produce higher thinking output pricing.\n\nAdditionally, Gemini 2.5 Flash can be configured through the 'maximum reasoning tokens' parameter, as described in the document (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
      "accounts/yi-01-ai/models/yi-large": "Yi-Large is one of the top large language models, ranking just behind GPT-4, Gemini 1.5 Pro, and Claude 3 Opus on the LMSYS benchmark leaderboard. It excels in multilingual capabilities, especially in Spanish, Chinese, Japanese, German, and French. Yi-Large is also user-friendly, adopting the same API definitions as OpenAI for easy integration.",
      "jamba-mini": "The most efficient model in its class, balancing speed and quality, with a smaller footprint.",
      "step-1-128k": "Balances performance and cost, suitable for general scenarios.",
      "gemini-2.5-flash-preview-04-17": "Gemini 2.5 Flash Preview is Google's highest cost-performance model, providing comprehensive functionality.",
      "gpt-4-turbo-2024-04-09": "The latest GPT-4 Turbo model has visual capabilities. Now, visual requests can use JSON mode and function calling. GPT-4 Turbo is a powerful version providing cost-effective support for multimodal tasks. It finds a balance between accuracy and efficiency, suitable for application scenarios requiring real-time interaction.",
      "cognitivecomputations/dolphin-mixtral-8x22b": "Dolphin Mixtral 8x22B is a model designed for instruction following, dialogue, and programming.",
      "grok-beta": "Has performance comparable to Grok 2, but with higher efficiency, speed, and functionality.",
      "qwen2.5-math-7b-instruct": "Qwen-Math model has powerful mathematical problem-solving capabilities.",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "Mixtral-8x7B Instruct (46.7B) provides high-reliability computational framework, suitable for large-scale data processing.",
      "qwen-plus-latest": "Tongyi Qianwen super large-scale language model enhanced version, supporting input in different languages including Chinese and English.",
      "Skylark2-pro-32k": "Second generation Skylark model, Skylark2-pro version has higher model accuracy, suitable for more complex text generation scenarios such as professional domain document generation, novel writing, high-quality translation, etc., with a 32k context window length.",
      "accounts/fireworks/models/llama-v3p3-70b-instruct": "Llama 3.3 70B Instruct is the December update of Llama 3.1 70B. This model improves upon Llama 3.1 70B (released in July 2024) with enhanced tool calling, multilingual text support, mathematics, and programming capabilities. The model reaches industry-leading levels in reasoning, mathematics, and instruction following, and can provide performance similar to 3.1 405B while having significant advantages in speed and cost.",
      "DeepSeek-R1-Distill-Qwen-32B": "DeepSeek-R1-Distill-Qwen-32B is a distilled model based on Qwen-2.5 series by DeepSeek-R1.",
      "llama3.1:405b": "Llama 3.1 is Meta's frontier model, supporting up to 405B parameters, applicable to complex dialogue, multilingual translation, and data analysis domains.",
      "yi-1.5-34b-chat": "Yi-1.5 is an upgraded version of Yi. It continues pre-training on Yi using a 500B Tokens high-quality corpus and is fine-tuned on 3M sample-balanced fine-tuning samples.",
      "llama-3.1-70b-versatile": "Llama 3.1 70B provides stronger AI reasoning capabilities, suitable for complex applications, supporting ultra-multi computational processing while ensuring efficiency and accuracy.",
      "google/gemini-pro-1.5": "Gemini 1.5 Pro combines the latest optimization techniques, bringing more efficient multimodal data processing capabilities.",
      "DeepSeek-R1-Distill-Qwen-14B": "DeepSeek-R1-Distill-Qwen-14B is a distilled model based on Qwen-2.5 series by DeepSeek-R1.",
      "SenseChat-Character-Pro": "Advanced character dialogue model version, 32K context length, comprehensively improved capabilities, supporting Chinese/English dialogue",
      "gpt-4-turbo-preview": "The latest GPT-4 Turbo model has visual capabilities. Now, visual requests can use JSON mode and function calling. GPT-4 Turbo is a powerful version providing cost-effective support for multimodal tasks. It finds a balance between accuracy and efficiency, suitable for application scenarios requiring real-time interaction.",
      "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": "Meta's instruction-tuned image reasoning model with 110 billion parameters. This model has been optimized for visual recognition, image reasoning, image caption generation, and common Q&A related to images. It can understand visual data like charts and graphs, and bridge the gap between vision and language by generating textual descriptions of image details.",
      "codellama:13b": "Code Llama is an LLM specialized in code generation and discussion, combining broad programming language support, suitable for developer environments.",
      "meta-llama/llama-3.1-70b-instruct": "Meta's latest generation of Llama 3.1 model series 70B (700 billion parameters) instruction-tuned version has been optimized for high-quality dialogue scenarios. In industry evaluations, compared with leading open-source models, it demonstrates strong performance. (Only open to enterprise real-name verified entities)",
      "mistral-large": "Mixtral Large is Mistral's flagship model, combining code generation, mathematics, and reasoning capabilities, supporting 128k context window.",
      "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": "DeepSeek-R1-Distill-Qwen-7B is a model obtained through knowledge distillation based on Qwen2.5-Math-7B. This model was fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating excellent reasoning capabilities. It performs exceptionally well in multiple benchmark tests, achieving 92.8% accuracy on MATH-500, 55.5% pass rate on AIME 2024, and a rating of 1189 on CodeForces, showing strong mathematical and programming capabilities for a 7B scale model.",
      "deepseek-v2": "DeepSeek V2 is an efficient Mixture-of-Experts language model, suitable for economically efficient processing needs.",
      "ernie-novel-8k": "Baidu's self-researched general large language model, with significant advantages in novel writing capabilities, also applicable to short plays, movies, and other scenarios.",
      "Qwen/Qwen2-72B-Instruct": "Qwen 2 Instruct (72B) provides accurate instruction understanding and response for enterprise applications.",
      "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": "Llama 3 70B Instruct Turbo provides superior language understanding and generation capabilities, suitable for the most demanding computational tasks.",
      "deepseek-r1-distill-llama-8b": "DeepSeek-R1-Distill-Llama-8B is a distilled model based on Llama3.1-8B-Base by DeepSeek-R1.",
      "internvl2.5-latest": "Our still-maintained InternVL2.5 version possesses excellent and stable performance. By default points to our latest released InternVL2.5 series models, currently pointing to internvl2.5-78b.",
      "ERNIE-Character-8K": "Baidu's self-researched vertical scenario large language model, suitable for game NPCs, customer service dialogue, dialogue character roleplay and other application scenarios, with clearer and more consistent character design, stronger instruction following capabilities, and better reasoning performance.",
      "step-2-16k": "Supports large-scale context interaction, suitable for complex dialogue scenarios.",
      "mistralai/mistral-7b-instruct": "Mistral 7B Instruct is a high-performance industry standard model with speed optimization and long context support.",
      "THUDM/GLM-Z1-Rumination-32B-0414": "GLM-Z1-Rumination-32B-0414 is a deep reasoning model with rumination capabilities (benchmarked against OpenAI's Deep Research). Unlike typical deep thinking models, rumination models use longer periods of deep thought to solve more open and complex problems.",
      "hunyuan-turbos-20250226": "hunyuan-TurboS pv2.1.2 fixed version with pre-training base token count upgrade; improved mathematical/logical/code thinking capabilities; enhanced Chinese/English general experience effects, including text creation, text understanding, knowledge Q&A, casual chat, etc.",
      "meta.llama3-1-70b-instruct-v1:0": "Updated version of Meta Llama 3.1 70B Instruct, including extended 128K context length, multilingualism, and improved reasoning capabilities. Llama 3.1 provides multilingual large language models (LLMs) that are a group of pre-trained and instruction-tuned generative models available in 8B, 70B, and 405B sizes (text input/output). Llama 3.1 instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many available open-source chat models in common industry benchmark tests. Llama 3.1 is intended for multilingual commercial and research use. The instruction-tuned text models are suitable for assistant-like chat, while the pre-trained models can adapt to various natural language generation tasks. Llama 3.1 models also support using their outputs to improve other models, including synthetic data generation and distillation. Llama 3.1 is an auto-regressive language model using an optimized transformer architecture. The adjusted versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "ministral-8b-latest": "Ministral 8B is Mistral's highly cost-effective edge model.",
      "hunyuan-large": "Hunyuan-large model has total parameters of about 389B, with activated parameters of about 52B, currently the largest parameter scale and best performing open-source MoE model with Transformer architecture.",
      "gemini-1.5-pro-exp-0801": "Gemini 1.5 Pro 0801 provides excellent multimodal processing capabilities, bringing greater flexibility to application development.",
      "gemma2-9b-it": "Gemma 2 9B is a model optimized for specific tasks and tool integration.",
      "gemini-2.5-pro-exp-03-25": "Gemini 2.5 Pro Experimental is Google's most advanced thinking model, capable of reasoning about complex problems in code, mathematics, and STEM fields, as well as using long context to analyze large datasets, code repositories, and documents.",
      "hunyuan-turbo-vision": "Hunyuan new generation vision-language flagship large model, adopting a new mixture-of-experts (MoE) structure, comprehensively improved compared to the previous generation model in image-text understanding-related basic recognition, content creation, knowledge Q&A, analysis and reasoning capabilities.",
      "whisper-1": "Universal speech recognition model, supporting multilingual speech recognition, speech translation, and language identification",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": "DeepSeek-R1-Distill-Qwen-7B is a model obtained through knowledge distillation based on Qwen2.5-Math-7B. This model was fine-tuned using 800,000 selected samples generated by DeepSeek-R1, demonstrating excellent reasoning capabilities. It performs exceptionally well in multiple benchmark tests, achieving 92.8% accuracy on MATH-500, 55.5% pass rate on AIME 2024, and a rating of 1189 on CodeForces, showing strong mathematical and programming capabilities for a 7B scale model.",
      "mixtral": "Mixtral is Mistral AI's expert model with open-source authority, providing support in code generation and language understanding.",
      "step-r1-v-mini": "This model is a reasoning large model with powerful image understanding capabilities, able to process image and text information, outputting text content after deep thinking. The model shows outstanding performance in visual reasoning domains while possessing first-tier mathematical, code, and text reasoning capabilities. Context length is 100k.",
      "accounts/fireworks/models/qwen2p5-72b-instruct": "Qwen2.5 is a series of decoder-only language models developed by the Qwen team and Alibaba Cloud, offering 0.5B, 1.5B, 4B, 7B, 14B, 32B, and 72B different parameter scales, including base and instruction-tuned versions.",
      "Qwen/Qwen2-1.5B-Instruct": "Qwen2-1.5B-Instruct is the instruction-tuned large language model in the Qwen2 series with 1.5B parameters. This model is based on the Transformer architecture and employs techniques such as SwiGLU activation function, attention QKV bias, and group-query attention. It shows excellent performance in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning across multiple benchmark tests, surpassing most open-source models. Compared to Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct shows significant performance improvements in MMLU, HumanEval, GSM8K, C-Eval, and IFEval tests, despite having slightly fewer parameters",
      "gemini-1.0-pro-002": "Gemini 1.0 Pro 002 (Tuning) provides excellent multimodal support, focusing on effective solutions for complex tasks.",
      "ERNIE-4.0-8K-Latest": "Baidu's self-researched flagship super large-scale language model, achieving comprehensive model capability upgrade compared to ERNIE 3.5, widely applicable to complex task scenarios in various domains; supports automatic connection to Baidu search plugin, ensuring Q&A information timeliness.",
      "taichu_o1": "taichu_o1 is a new generation reasoning large model that achieves human-like reasoning chains through multimodal interaction and reinforcement learning, supporting complex decision inference, demonstrating the reasoning path of model inference while maintaining high-precision output, suitable for strategy analysis and deep thinking scenarios.",
      "SenseChat-Character": "Character dialogue standard version model, 8K context length, high response speed",
      "360gpt-turbo": "360GPT Turbo provides powerful computation and dialogue capabilities, possessing excellent semantic understanding and generation efficiency, an ideal intelligent assistant solution for enterprises and developers.",
      "qwen-max": "Tongyi Qianwen trillion-level super large-scale language model, supporting input in different languages including Chinese and English, currently the API model behind the Tongyi Qianwen 2.5 product version.",
      "yi-lightning": "Latest high-performance model, ensuring high-quality output while significantly improving reasoning speed.",
      "gpt-4o-2024-08-06": "ChatGPT-4o is a dynamic model, updated in real-time to maintain the current latest version. It combines powerful language understanding and generation capabilities, suitable for large-scale application scenarios including customer service, education, and technical support.",
      "step-1.5v-mini": "This model has powerful video understanding capabilities.",
      "command-r-plus-08-2024": "Command R+ is an instruction-following dialogue model that demonstrates higher quality, more reliability in language tasks, and has longer context length compared to previous models. It is best suited for complex RAG workflows and multi-tool usage.",
      "Qwen/Qwen2.5-14B-Instruct": "Qwen2.5-14B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 14B model shows significant improvements in coding and mathematical domains. The model also provides multilingual support, covering over 129 languages including Chinese and English. The model shows notable improvements in instruction following, understanding structured data, and generating structured output (especially JSON).",
      "deepseek-r1": "DeepSeek-R1 introduces cold-start data before reinforcement learning (RL), performing comparably to OpenAI-o1 in mathematics, code, and reasoning tasks.",
      "openai/gpt-4o-mini": "GPT-4o mini is OpenAI's latest model released after GPT-4 Omni, supporting both image input and text output. As their most advanced small model, it is much more affordable than other recent frontier models and over 60% more cost-effective than GPT-3.5 Turbo. It maintains cutting-edge intelligence while offering remarkable value for money. GPT-4o mini achieved a score of 82% in MLU tests and currently ranks higher than GPT-4 in chat preferences.",
      "openai/o4-mini": "o4-mini is optimized for fast and effective reasoning, demonstrating exceptional efficiency and performance in coding and visual tasks.",
      "qwen/qwen2.5-coder-7b-instruct": "Qwen2.5-Coder-7B-Instruct is the latest version of Alibaba Cloud's code-specific large language model series. Building upon Qwen2.5, this model has been trained on 55 billion tokens, significantly enhancing its code generation, reasoning, and correction capabilities. It not only improves coding abilities but also maintains advantages in mathematics and general capabilities. The model provides a more comprehensive foundation for practical applications like code intelligence systems.",
      "ernie-x1-32k-preview": "The Wenxin large model X1 features enhanced capabilities in comprehension, planning, reflection, and evolution. As a more comprehensive deep thinking model, Wenxin X1 combines accuracy, creativity, and eloquence, demonstrating exceptional performance in Chinese knowledge Q&A, literary creation, article writing, daily conversation, logical reasoning, complex calculations, and tool calling.",
      "deepseek/deepseek-r1/community": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, possessing extremely strong reasoning capabilities, especially achieving comparable performance to OpenAI's o1 model in mathematics, programming, and reasoning tasks.",
      "hunyuan-translation-lite": "Hunyuan translation model supports natural language dialogue-style translation; supports mutual translation between 15 languages including Chinese and English, Japanese, French, Portuguese, Spanish, Turkish, Russian, Arabic, Korean, Italian, German, Vietnamese, Malay, Indonesian.",
      "SenseChat-128K": "Base version model (V4), 128K context length, showing excellent performance in long text understanding and generation tasks",
      "internlm/internlm2_5-20b-chat": "InternLM2.5-20B-Chat is an open-source large-scale dialogue model developed based on the InternLM2 architecture. This model has 200 billion parameters and performs excellently in mathematical reasoning, surpassing same-scale Llama3 and Gemma2-27B models. InternLM2.5-20B-Chat shows significant improvements in tool calling capabilities, supporting information collection and analysis from hundreds of web pages, and possessing stronger instruction understanding, tool selection, and result reflection capabilities. It is suitable for building complex agents and can perform multiple rounds of tool calling to complete complex tasks.",
      "mathstral": "Mathstral is designed for scientific research and mathematical reasoning, providing effective computational capabilities and result interpretation.",
      "moonshot-v1-32k": "Moonshot V1 32K provides medium-length context processing capabilities, able to process 32,768 tokens, particularly suitable for generating various long documents and complex dialogues, applicable in content creation, report generation, and dialogue systems domains.",
      "360gpt2-pro": "360GPT2 Pro is an advanced natural language processing model launched by 360 company, possessing superior text generation and understanding capabilities, particularly excelling in generation and creation domains, able to handle complex language transformation and character roleplay tasks.",
      "anthropic.claude-3-sonnet-20240229-v1:0": "Anthropic's Claude 3 Sonnet achieves an ideal balance between intelligence and speed 鈥?especially suitable for enterprise workloads. It provides maximum utility at a lower price than competitors and is designed to be a reliable, resilient workhorse suitable for scaled AI deployments. Claude 3 Sonnet can process images and return text outputs, with a 200K context window."
    }
  }
}